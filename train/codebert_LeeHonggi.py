import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
import pandas as pd
import numpy as np
import os
from transformers import (
    RobertaConfig,
    RobertaModel,
    PreTrainedTokenizerFast,
    get_linear_schedule_with_warmup
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# Dataset
# ============================================================================
class CodePairDataset(Dataset):
    """
    ì½”ë“œ í˜ì–´ ìœ ì‚¬ë„ ë°ì´í„°ì…‹
    """
    def __init__(self, df, tokenizer, max_length=512):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        code1 = str(row['code1_processed'])
        code2 = str(row['code2_processed'])
        label = int(row['similar'])

        # [CLS] code1 [SEP] code2 [SEP] í˜•íƒœë¡œ ê²°í•©
        encoding = self.tokenizer(
            code1,
            code2,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }


# ============================================================================
# Model
# ============================================================================
class CodeSimilarityClassifier(nn.Module):
    """
    CodeBERT + Classification Head
    """
    def __init__(self, encoder, hidden_size=768, num_labels=2, dropout=0.1):
        super(CodeSimilarityClassifier, self).__init__()
        
        self.encoder = encoder
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        # CodeBERT Encoding
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # [CLS] tokenì˜ hidden state ì¶”ì¶œ
        pooled_output = outputs.last_hidden_state[:, 0, :]
        
        # Classification
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        
        return logits


# ============================================================================
# Training Functions
# ============================================================================
def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):
    """í•œ ì—í¬í¬ í•™ìŠµ"""
    model.train()
    total_loss = 0
    predictions = []
    true_labels = []
    
    progress_bar = tqdm(dataloader, desc="Training")
    
    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        
        # Forward pass
        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        # Metrics
        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())
        
        # Progress bar update
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'lr': f'{scheduler.get_last_lr()[0]:.2e}'
        })
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)
    
    return avg_loss, accuracy, f1


def evaluate(model, dataloader, criterion, device):
    """í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)
            
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)
    
    return avg_loss, accuracy, f1


# ============================================================================
# Main Training Pipeline
# ============================================================================
def main():
    """ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    
    # ========================================================================
    # 1. í™˜ê²½ ì„¤ì •
    # ========================================================================
    print("=" * 70)
    print("CodeBERT Fine-tuning - ì½”ë“œ ìœ ì‚¬ë„ ë¶„ë¥˜")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    # ========================================================================
    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„°
    # ========================================================================
    BATCH_SIZE = 64
    MAX_LENGTH = 512
    LEARNING_RATE = 2e-5
    EPOCHS = 3
    WARMUP_STEPS = 100
    
    print(f"\ní•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  Max Length: {MAX_LENGTH}")
    print(f"  Learning Rate: {LEARNING_RATE}")
    print(f"  Epochs: {EPOCHS}")
    
    # ========================================================================
    # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
    # ========================================================================
    print("\n" + "=" * 70)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ")
    print("=" * 70)
    
    TOKENIZER_DIR = "./data/tokenizer"
    
    try:
        tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_DIR)
        print(f"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {TOKENIZER_DIR}")
        print(f"  Vocab Size: {len(tokenizer):,}")
        print(f"  Special Tokens: {tokenizer.special_tokens_map}")
    except Exception as e:
        print(f"âœ— í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"  ê²½ë¡œ í™•ì¸: {TOKENIZER_DIR}")
        print(f"  ë¨¼ì € tokenizers/bpe_tokenizer_LeeHonggi.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # ========================================================================
    # 4. CodeBERT ëª¨ë¸ ë¡œë“œ
    # ========================================================================
    print("\n" + "=" * 70)
    print("CodeBERT ëª¨ë¸ ë¡œë“œ")
    print("=" * 70)
    
    MODEL_NAME = "microsoft/codebert-base"
    
    config = RobertaConfig.from_pretrained(MODEL_NAME)
    print(f"âœ“ Config ë¡œë“œ ì™„ë£Œ")
    print(f"  Vocab Size: {config.vocab_size:,}")
    print(f"  Hidden Size: {config.hidden_size}")
    print(f"  Num Layers: {config.num_hidden_layers}")
    print(f"  Attention Heads: {config.num_attention_heads}")
    
    encoder = RobertaModel.from_pretrained(MODEL_NAME)
    print(f"âœ“ CodeBERT Encoder ë¡œë“œ ì™„ë£Œ")
    print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # ========================================================================
    # 5. ë°ì´í„° ë¡œë“œ
    # ========================================================================
    print("\n" + "=" * 70)
    print("í•™ìŠµ ë°ì´í„° ë¡œë“œ")
    print("=" * 70)
    
    TRAIN_DATA_PATH = "./data/train_pairs.parquet"
    
    try:
        train_df = pd.read_parquet(TRAIN_DATA_PATH)
        print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(train_df):,}ê°œ ìƒ˜í”Œ")
        print(f"\nLabel ë¶„í¬:")
        print(train_df['similar'].value_counts())
    except Exception as e:
        print(f"âœ— ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"  ê²½ë¡œ í™•ì¸: {TRAIN_DATA_PATH}")
        return
    
    # ========================================================================
    # 6. ë°ì´í„° ë¶„í• 
    # ========================================================================
    print("\n" + "=" * 70)
    print("ë°ì´í„° ë¶„í•  (Train/Val)")
    print("=" * 70)
    
    train_df_split, val_df = train_test_split(
        train_df,
        test_size=0.1,
        random_state=42,
        stratify=train_df['similar']
    )
    
    print(f"âœ“ ë°ì´í„° ë¶„í•  ì™„ë£Œ")
    print(f"  Train: {len(train_df_split):,}ê°œ")
    print(f"  Val: {len(val_df):,}ê°œ")
    
    # Dataset & DataLoader ìƒì„±
    train_dataset = CodePairDataset(train_df_split, tokenizer, MAX_LENGTH)
    val_dataset = CodePairDataset(val_df, tokenizer, MAX_LENGTH)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    
    print(f"âœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ")
    print(f"  Train Batches: {len(train_loader)}")
    print(f"  Val Batches: {len(val_loader)}")
    
    # ========================================================================
    # 7. ëª¨ë¸ ì´ˆê¸°í™”
    # ========================================================================
    print("\n" + "=" * 70)
    print("ëª¨ë¸ ì´ˆê¸°í™”")
    print("=" * 70)
    
    model = CodeSimilarityClassifier(
        encoder=encoder,
        hidden_size=config.hidden_size,
        num_labels=2,
        dropout=0.1
    )
    model.to(device)
    
    print(f"âœ“ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    print(f"  ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # Loss, Optimizer, Scheduler
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
    
    total_steps = len(train_loader) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=WARMUP_STEPS,
        num_training_steps=total_steps
    )
    
    print(f"âœ“ Optimizer & Scheduler ì„¤ì • ì™„ë£Œ")
    print(f"  Total Steps: {total_steps}")
    print(f"  Warmup Steps: {WARMUP_STEPS}")
    
    # ========================================================================
    # 8. í•™ìŠµ ì‹œì‘
    # ========================================================================
    print("\n" + "=" * 70)
    print("í•™ìŠµ ì‹œì‘")
    print("=" * 70)
    
    best_val_f1 = 0
    history = {
        'train_loss': [],
        'train_acc': [],
        'train_f1': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': []
    }
    
    for epoch in range(EPOCHS):
        print(f"\n{'=' * 70}")
        print(f"Epoch {epoch + 1}/{EPOCHS}")
        print(f"{'=' * 70}")
        
        # Train
        train_loss, train_acc, train_f1 = train_epoch(
            model, train_loader, optimizer, scheduler, criterion, device
        )
        
        # Validate
        val_loss, val_acc, val_f1 = evaluate(
            model, val_loader, criterion, device
        )
        
        # Save history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['train_f1'].append(train_f1)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        # Print results
        print(f"\nğŸ“Š Epoch {epoch + 1} Results:")
        print(f"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        print(f"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}")
        
        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            model_save_path = './models/best_codebert_LeeHonggi.pt'
            os.makedirs('./models', exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  âœ… Best model saved! (F1: {best_val_f1:.4f})")
    
    # ========================================================================
    # 9. í•™ìŠµ ì™„ë£Œ
    # ========================================================================
    print("\n" + "=" * 70)
    print("í•™ìŠµ ì™„ë£Œ!")
    print("=" * 70)
    print(f"Best Validation F1: {best_val_f1:.4f}")
    print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: ./models/best_codebert_LeeHonggi.pt")
    print("=" * 70)


if __name__ == "__main__":
    main()
