# Code Similarity Project

> **Team**: I'm fine tuning  
> **Project Period**: 2025.10.17 ~ 2025.10.23  
> **Status**: âœ… Completed

## ğŸ“‹ Overview

CodeNet ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ pretrained ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ë°ì´ì½˜ í•™ìŠµ ë°ì´í„°ë¡œ fine-tuningì„ ì§„í–‰í•˜ì—¬ ì½”ë“œ ìœ ì‚¬ë„ íŒë³„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ¯ Project Goals

1. **Pretraining**: CodeNet ë°ì´í„°ì…‹ìœ¼ë¡œ pretrained ëª¨ë¸ êµ¬ì¶• âœ…
2. **Fine-tuning**: ë°ì´ì½˜ ê²½ì§„ëŒ€íšŒ ë°ì´í„°ë¡œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • âœ…
3. **Evaluation**: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¶„ì„ âœ…

## ğŸ¤– Models

### êµ¬í˜„ëœ ëª¨ë¸

| ëª¨ë¸ëª… | ë‹´ë‹¹ì | êµ¬í˜„ ë°©ì‹ | Public Score | Private Score |
|--------|--------|-----------|--------------|---------------|
| **CodeBERT (HuggingFace)** | ì´í™ê¸° | microsoft/codebert-base | **92.84%** | **92.86%** |
| **RoBERTa-small** | í™©í˜¸ì„± | Custom Implementation | **91.67%** | **91.64%** |
| **Custom BERT** | ì¡°ë³‘ë¥  | From Scratch | **86.33%** | **86.32%** |
| **RoleBERT** | ì˜¤ì •íƒ | Role Embedding | **93.17%** | **93.18%** |
| **Contrastive Learning** | ì´ì„œìœ¨ | Dual Encoder + AST | **62.18%** | **62.03%** |

### ë°ì´í„° ì „ì²˜ë¦¬

```bash
python preprocessing/preprocess_corpus.py
```

### ëª¨ë¸ë³„ íŠ¹ì§•

#### 1. CodeBERT (HuggingFace) - ì´í™ê¸°

- **í•µì‹¬**: microsoft/codebert-base ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©
- **í•™ìŠµ ë°©ì‹**: MLM (Masked Language Modeling) + RTD (Replaced Token Detection)
- **íŠ¹ì§•**: 
  - ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ ë°ì´í„°ì˜ íš¨ê³¼
  - Transfer Learning ìµœëŒ€ í™œìš©

**ì‚¬ìš©ë²•:**
```bash
python tokenizers/bpe_tokenizer_LeeHonggi.py
python train/codebert_LeeHonggi.py
```

#### 2. RoBERTa-small - í™©í˜¸ì„±

- **í•µì‹¬**: RoBERTa ìµœì í™” ê¸°ë²• ì ìš©
- **Tokenizer**: Unigram (32k vocab)
- **íŠ¹ì§•**:
  - íš¨ê³¼ì ì¸ ì „ì²˜ë¦¬ (ì¢Œì¸¡ ì ˆë‹¨, ìµœì†Œí™”)
  - êµ¬ì¡°/ë¦¬í„°ëŸ´ ë§ˆì»¤ í™œìš© (`<str>`, `<num>`, `<indent>`)
  - Custom ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ ğŸ¥ˆ

**ì‚¬ìš©ë²•:**
```bash
# 1. Unigram í† í¬ë‚˜ì´ì € í•™ìŠµ (ë¨¼ì € ì‹¤í–‰)
python tokenizers/unigram_tokenizer_HwangHosung.py

# 2-A. ë¡œì»¬ì—ì„œ í”„ë¦¬íŠ¸ë ˆì´ë‹
python train/pretrain_roberta_HwangHosung.py --mode train

# 2-B. ë˜ëŠ” í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ í™•ì¸
python train/pretrain_roberta_HwangHosung.py --mode load

# 3. ë¡œì»¬ í”„ë¦¬íŠ¸ë ˆì´ë‹ ëª¨ë¸ ì‚¬ìš©
python train/finetune_roberta_HwangHosung.py --model local

# 3-1. í—ˆê¹…í˜ì´ìŠ¤ í”„ë¦¬íŠ¸ë ˆì´ë‹ ëª¨ë¸ ì‚¬ìš©
python train/finetune_roberta_HwangHosung.py --model huggingface

# 3-2. í—ˆê¹…í˜ì´ìŠ¤ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œë§Œ
python train/finetune_roberta_HwangHosung.py --model huggingface --mode load

# 4. ì¶”ë¡  - í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©
python inference/inference_HwangHosung.py \
    --model huggingface \
    --test ./data/test.csv \
    --output ./inference/submission.csv

# 4-1. ì¶”ë¡  - ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
python inference/inference_HwangHosung.py \
    --model local \
    --test ./data/test.csv \
    --output ./inference/submission_local.csv
```

#### 3. Custom BERT - ì¡°ë³‘ë¥ 

- **í•µì‹¬**: ì²˜ìŒë¶€í„° êµ¬ì¶•í•œ BERT ì•„í‚¤í…ì²˜
- **í•™ìŠµ ë°©ì‹**: MLM (Masked Language Modeling)
- **íŠ¹ì§•**:
  - ê¸°ë³¸ì— ì¶©ì‹¤í•œ êµ¬í˜„
  - Baseline ì—­í• 
  - í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

**ì‚¬ìš©ë²•:**
```bash
# 1-A. custombert ì‚¬ìš©
python train/pretrain_custombert_ByeongRyul.py
# 1-B. electra ì‚¬ìš©
python train/pretrain_electra_ByeongRyul.py
# 2. íŒŒì¸íŠœë‹
python train/finetune_ByeongRyul.py
# 3. ì¶”ë¡ 
python inference/inference_ByeongRyul.py
```
#### 4. RoleBERT - ì˜¤ì •íƒ

- **í•µì‹¬**: Role Embedding ì¶”ê°€
- **í•™ìŠµ ë°©ì‹**: MLM + Role Prediction
- **íŠ¹ì§•**:
  - êµ¬ë¬¸ ì •ë³´(KEYWORD, IDENTIFIER ë“±) ëª…ì‹œì  í™œìš©
  - AST ê¸°ë°˜ ì½”ë“œ ì •ê·œí™”
  - í˜ì‹ ì  ì‹œë„

**ì‚¬ìš©ë²•:**
```bash
python train/pretrain_rolebert_JeongTak.py
python train/finetune_JeongTak.py
```

#### 5. Contrastive Learning - ì´ì„œìœ¨

- **í•µì‹¬**: MoCo ê¸°ë°˜ ë“€ì–¼ ì¸ì½”ë”
- **êµ¬ì¡°**: Text Encoder + AST-GNN
- **íŠ¹ì§•**:
  - AST êµ¬ì¡° ì •ë³´ í™œìš©
  - Hard Negative Mining
  - ì‹¤í—˜ì  ì ‘ê·¼

**ì‚¬ìš©ë²•:**
```bash
python tokenizers/unigram_tokenizer_LeeSeoYul.py
python train/pretrain_LeeSeoYul.py
python inference/inference_LeeSeoYul.py
```

## ğŸ”§ Preprocessing & Tokenization

### ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ âœ…

#### ì²˜ë¦¬ ë‹¨ê³„ (ìˆœì°¨ì  ì ìš©)

1. **ì£¼ì„ ì œê±°** (`remove_comments`)
2. **ì‹ë³„ì ì •ê·œí™”** (`alpha_rename`) - ë³€ìˆ˜ëª…/í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… ìµëª…í™”
3. **ê³µë°± ì •ê·œí™”** - ì—°ì† ê°œí–‰ì„ ìµœëŒ€ 2ê°œë¡œ ì œí•œ
4. **ì½”ë“œ í¬ë§·íŒ…** (`black`) - 88ì ì¤„ ê¸¸ì´ ê¸°ì¤€
5. **ì¤‘ë³µ ì œê±°** - SHA1 í•´ì‹œ ê¸°ë°˜

### í† í¬ë‚˜ì´ì € êµ¬í˜„ âœ…

ê° ëª¨ë¸ë³„ë¡œ ìµœì í™”ëœ í† í¬ë‚˜ì´ì €ë¥¼ `/tokenizers` í´ë”ì— êµ¬í˜„:

#### 1. Unigram (í™©í˜¸ì„± - RoBERTa, ì´ì„œìœ¨ - Contrastive)

- í¬ê·€ ì‹ë³„ì ì²˜ë¦¬ì— ì•ˆì •ì 
- ì†Œê·œëª¨ ë°ì´í„°ì— ë¹ ë¥¸ ìˆ˜ë ´
- íŠ¹ìˆ˜ í† í°: `<indent>`, `<dedent>`, `<str>`, `<num>` ë“±

#### 2. BPE (ì¡°ë³‘ë¥ , ì˜¤ì •íƒ, ì´í™ê¸°)

- RoBERTa/CodeBERT í‘œì¤€
- vocab_size: 32k-50k
- ì•ˆì •ì  ì„±ëŠ¥

#### 3. SentencePiece (ì¡°ë³‘ë¥ )

- Custom BERT êµ¬í˜„
- vocab_size: 32k

## ğŸ“‚ Project Structure

```
code-similarity/
â”œâ”€â”€ README.md                              # í”„ë¡œì íŠ¸ ë©”ì¸ ë¬¸ì„œ
â”œâ”€â”€ .gitignore                             # Git ì œì™¸ íŒŒì¼ ì„¤ì •
â”œâ”€â”€ preprocessing/                         # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â””â”€â”€ preprocess_corpus.py              # ì½”í¼ìŠ¤ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ tokenizers/                            # í† í¬ë‚˜ì´ì € êµ¬í˜„
â”‚   â”œâ”€â”€ unigram_tokenizer_HwangHosung.py  # Unigram (í™©í˜¸ì„±)
â”‚   â”œâ”€â”€ unigram_tokenizer_LeeSeoYul.py    # Unigram (ì´ì„œìœ¨)
â”‚   â”œâ”€â”€ bpe_tokenizer_LeeHonggi.py        # BPE (ì´í™ê¸°)
â”‚   â””â”€â”€ sentencepiece_tokenizer.py        # SentencePiece (ì¡°ë³‘ë¥ )
â”œâ”€â”€ train/                                 # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ codebert_LeeHonggi.py             # CodeBERT í•™ìŠµ (ì´í™ê¸°)
â”‚   â”œâ”€â”€ pretrain_roberta_HwangHosung.py   # RoBERTa Pretrain (í™©í˜¸ì„±)
â”‚   â”œâ”€â”€ finetune_roberta_HwangHosung.py   # RoBERTa Finetune (í™©í˜¸ì„±)
â”‚   â”œâ”€â”€ pretrain_rolebert_JeongTak.py     # RoleBERT Pretrain (ì˜¤ì •íƒ)
â”‚   â”œâ”€â”€ finetune_JeongTak.py              # RoleBERT Finetune (ì˜¤ì •íƒ)
â”‚   â””â”€â”€ pretrain_LeeSeoYul.py             # Contrastive Pretrain (ì´ì„œìœ¨)
â”‚   â””â”€â”€ pretrain_custombert_ByeongRyul.py # Custombert Pretrain (ì¡°ë³‘ë¥ )
â”‚   â””â”€â”€ pretrain_electra_ByeongRyul.py    # Electra Pretrain (ì¡°ë³‘ë¥ )
â”‚   â””â”€â”€ finetune_ByeongRyul.py            # Finetune (ì¡°ë³‘ë¥ )
â”œâ”€â”€ inference/                             # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ inference_HwangHosung.py          # RoBERTa ì¶”ë¡  (í™©í˜¸ì„±)
â”‚   â””â”€â”€ inference_LeeSeoYul.py            # Contrastive ì¶”ë¡  (ì´ì„œìœ¨)
â”‚   â””â”€â”€ inference_ByeongRyul.py            # ë‘ê°œ ëª¨ë¸ ì¶”ë¡  (ì¡°ë³‘ë¥ )
â”œâ”€â”€ models/                                # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
â”œâ”€â”€ data/                                  # ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ train.csv                         # í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ test.csv                          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â””â”€â”€ code_corpus_processed.parquet     # ì „ì²˜ë¦¬ëœ ì½”í¼ìŠ¤
â”œâ”€â”€ meeting-notes/                         # íšŒì˜ë¡ ëª¨ìŒ
â”‚   â”œâ”€â”€ 2025-10-17.md                     # í‚¥ì˜¤í”„ íšŒì˜ë¡
â”‚   â”œâ”€â”€ 2025-10-20.md                     # ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì € ê²°ì •
â”‚   â”œâ”€â”€ 2025-10-21.md                     # ì „ì²˜ë¦¬ ì™„ë£Œ ë° í•™ìŠµ ì‹œì‘
â”‚   â””â”€â”€ 2025-10-22.md                     # ìµœì¢… ê²°ê³¼ ê³µìœ 
â”œâ”€â”€ notebooks/                             # ì‹¤í—˜ ë…¸íŠ¸ë¶
â””â”€â”€ docs/                                  # ë¬¸ì„œ ë° ìë£Œ
    â””â”€â”€ ë°œí‘œìë£Œ.md                        # ìµœì¢… ë°œí‘œìë£Œ
```

## ğŸ“Š Datasets

### 1. CodeNet (Pretraining)

- **ì¶œì²˜**: IBM Research
- **ì„¤ëª…**: ëŒ€ê·œëª¨ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ ë° ì†”ë£¨ì…˜ ë°ì´í„°ì…‹
- **ë§í¬**: [CodeNet GitHub](https://github.com/IBM/Project_CodeNet/blob/main/README.md#directory-structure-and-naming-convention)
- **ìš©ë„**: Pretrained ëª¨ë¸ êµ¬ì¶•
- **ì²˜ë¦¬ ê²°ê³¼**: 604,124ê°œ ì¤‘ë³µ ì œê±° â†’ 2,639,300ê°œ ìƒ˜í”Œ

### 2. Dacon - ì›”ê°„ ì½”ë“œ ìœ ì‚¬ì„± íŒë‹¨ AI ê²½ì§„ëŒ€íšŒ (Fine-tuning)

- **ì¶œì²˜**: ë°ì´ì½˜
- **ì„¤ëª…**: ì½”ë“œ ìœ ì‚¬ë„ íŒë³„ íƒœìŠ¤í¬ ë°ì´í„°
- **ë§í¬**: [ëŒ€íšŒ í˜ì´ì§€](https://dacon.io/competitions/official/235900/overview/description)
- **ìš©ë„**: Fine-tuning ë° í‰ê°€

## ğŸ“š References

### í•µì‹¬ ë…¼ë¬¸

1. **CodeBERT**: CodeBERT: A Pre-Trained Model for Programming and Natural Languages (EMNLP 2020)
2. **RoBERTa**: RoBERTa: A Robustly Optimized BERT Pretraining Approach (arXiv 2019)
3. **MoCo**: Momentum Contrast for Unsupervised Visual Representation Learning (CVPR 2020)
4. **ELECTRA**: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (ICLR 2020)

## âœ… Task List

### Phase 1: ëª¨ë¸ ì´í•´ âœ…

- [x] CodeBERT ì•„í‚¤í…ì²˜ í•™ìŠµ
- [x] CodeBERT ì‚¬ì „ í•™ìŠµ ë°©ì‹ ì´í•´
- [x] Fine-tuning ë°©ë²•ë¡  ì—°êµ¬
- [x] ê°ì í•™ìŠµ ë‚´ìš© ì •ë¦¬ ë° ê³µìœ 

### Phase 2: ë°ì´í„° ì¤€ë¹„ âœ…

- [x] ë°ì´í„° ì „ì²˜ë¦¬ ë°©ì‹ ê²°ì • (ì¤‘ë³µ ì œê±°)
- [x] í† í¬ë‚˜ì´ì € ì˜µì…˜ ê²€í†  (WordPiece vs BPE)
- [x] ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [x] ì „ì²˜ë¦¬ ì™„ë£Œ ë° ë©”íƒ€ë°ì´í„° ìƒì„±

### Phase 3: ëª¨ë¸ í•™ìŠµ âœ…

- [x] Fine-tuning í™˜ê²½ êµ¬ì¶•
- [x] ê° ëª¨ë¸ë³„ í•™ìŠµ ì§„í–‰
  - [x] CodeBERT Fine-tuning (ì´í™ê¸°)
  - [x] RoBERTa-small Fine-tuning (í™©í˜¸ì„±)
  - [x] Custom BERT í•™ìŠµ (ì¡°ë³‘ë¥ )
  - [x] RoleBERT í•™ìŠµ (ì˜¤ì •íƒ)
  - [x] Contrastive Learning í•™ìŠµ (ì´ì„œìœ¨)
- [x] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### Phase 4: í‰ê°€ ë° ë¶„ì„ âœ…

- [x] ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- [x] ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
- [x] ìµœì¢… ë³´ê³ ì„œ ì‘ì„±

## ğŸ’¡ Key Insights

### í•µì‹¬ ì¸ì‚¬ì´íŠ¸

#### 1. ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ì¤‘ìš”ì„±

- **HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸** (ì´í™ê¸°) > Custom ìµœì í™” (í™©í˜¸ì„±) > Custom Baseline (ì¡°ë³‘ë¥ )
- ëŒ€ê·œëª¨ ë°ì´í„° í•™ìŠµ íš¨ê³¼ê°€ ì••ë„ì 
- Transfer Learningì˜ í˜

#### 2. ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±

- í™©í˜¸ì„±ì˜ ì „ì²˜ë¦¬ ë°©ë²•: **+4.5%p** ì„±ëŠ¥ í–¥ìƒ
- ì¢Œì¸¡ ì ˆë‹¨(Left Truncation) ì „ëµ íš¨ê³¼ì 
- ê³¼ë„í•œ ì •ì œë³´ë‹¤ ì ì ˆí•œ ìµœì†Œí™”ê°€ ì¤‘ìš”

#### 3. ëª¨ë¸ ì•„í‚¤í…ì²˜

- ë³µì¡í•œ êµ¬ì¡° < **ìµœì í™”ëœ ë‹¨ìˆœ êµ¬ì¡°**
- í‘œì¤€ Transformer: ì•ˆì •ì , íš¨ìœ¨ì 
- êµ¬ì¡° ì •ë³´ëŠ” ë³´ì¡°ì ìœ¼ë¡œ í™œìš©

#### 4. í•™ìŠµ ì „ëµ

- AdamW + Warmup Scheduler í‘œì¤€
- Learning Rate: 2e-5 ~ 6e-4
- Full Fine-tuningì´ íš¨ê³¼ì 

#### 5. Tokenizer ì„ íƒ

- **Unigram**: í¬ê·€ ì‹ë³„ì ì•ˆì •ì , ì†Œê·œëª¨ ë°ì´í„° ì í•©
- **BPE**: í‘œì¤€ì , ëŒ€ê·œëª¨ vocab ì•ˆì •ì 
- Special Tokens ì„¤ê³„ ì¤‘ìš” (`<str>`, `<num>`, `<indent>`)

## ğŸ‘¥ Team Contributions

| íŒ€ì› | ì—­í•  | ì£¼ìš” ê¸°ì—¬ |
|------|------|-----------|
| **ì´í™ê¸°** | CodeBERT (HF) | HF ëª¨ë¸ í™œìš© |
| **í™©í˜¸ì„±** | RoBERTa-small | ì „ì²˜ë¦¬ ìµœì í™”, Custom ì¤‘ ìµœê³  |
| **ì¡°ë³‘ë¥ ** | Custom BERT | Baseline ì œê³µ, ê¸°ë³¸ êµ¬í˜„ |
| **ì˜¤ì •íƒ** | RoleBERT | í˜ì‹ ì  Role Embedding, ì½”ë“œ ì •ê·œí™” |
| **ì´ì„œìœ¨** | Contrastive | ëŒ€ì¡°í•™ìŠµ + AST, ì‹¤í—˜ì  ì ‘ê·¼ |

## ğŸ“ Meeting Notes

í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ íšŒì˜ ë‚´ìš©:

- [2025-10-17: í‚¥ì˜¤í”„ ë¯¸íŒ…](./meeting-notes/2025-10-17.md)
- [2025-10-20: ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì € ê²°ì •](./meeting-notes/2025-10-20.md)
- [2025-10-21: ì „ì²˜ë¦¬ ì™„ë£Œ ë° í•™ìŠµ ì‹œì‘](./meeting-notes/2025-10-21.md)
- [2025-10-22: ìµœì¢… ê²°ê³¼ ê³µìœ  ë° ë¶„ì„](./meeting-notes/2025-10-22.md)

## ğŸ“„ Documentation

### ìµœì¢… ë³´ê³ ì„œ

í”„ë¡œì íŠ¸ì˜ ìƒì„¸í•œ ë¶„ì„ê³¼ ì¸ì‚¬ì´íŠ¸ëŠ” ë‹¤ìŒ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**[ë°œí‘œìë£Œ.md](./docs/ë°œí‘œìë£Œ.md)**

ì´ ë³´ê³ ì„œëŠ” ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤:

- ê° ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ ìƒì„¸ ë¶„ì„
- ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì € ë¹„êµ
- í•™ìŠµ ì „ëµ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹„êµ
- ì„±ëŠ¥ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
- í–¥í›„ ê°œì„  ë°©í–¥

## ğŸš€ Lessons Learned

### ì„±ê³µ ìš”ì¸

1. **ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©** - Transfer Learningì˜ ìœ„ë ¥
2. **íš¨ê³¼ì ì¸ ì „ì²˜ë¦¬** - ì¢Œì¸¡ ì ˆë‹¨, ì ì ˆí•œ ìµœì†Œí™”
3. **ì•ˆì •ì ì¸ í•™ìŠµ ì „ëµ** - AdamW, Warmup, Scheduler
4. **ë‹¤ì–‘í•œ ì ‘ê·¼ë²• ì‹œë„** - 5ê°€ì§€ ë‹¤ë¥¸ ë°©ë²•ë¡ 

### ê°œì„  ê°€ëŠ¥ ì˜ì—­

1. **ì•™ìƒë¸”** - ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
2. **ë°ì´í„° ì¦ê°•** - ë” ë§ì€ í•™ìŠµ ë°ì´í„° í™œìš©
3. **êµ¬ì¡° ì •ë³´ í†µí•©** - AST/Role ì •ë³´ì˜ ê²½ëŸ‰ í†µí•©
4. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** - ë” ì„¸ë°€í•œ ìµœì í™”

## ğŸ“Œ Contact

í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ íŒ€ì›ì—ê²Œ ì—°ë½í•´ì£¼ì„¸ìš”.

**Team**: I'm fine tuning

---

**Project Period**: 2025.10.17 ~ 2025.10.23  
**Last Updated**: 2025.10.24  
**Status**: âœ… Completed
