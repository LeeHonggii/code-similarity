{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAltZhyXsVYK",
        "outputId": "e257d9b4-01a6-4aca-c70c-b8c833a35249"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxGub_-bS_5K",
        "outputId": "ac17589b-6e8f-4a96-9a14-1b42bb09fb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Checkpoint] Loaded backbone params: 0\n",
            "Epoch 1 Step 200/10125 | loss=0.6840\n",
            "Epoch 1 Step 400/10125 | loss=0.6473\n",
            "Epoch 1 Step 600/10125 | loss=0.5894\n",
            "Epoch 1 Step 800/10125 | loss=0.5445\n",
            "Epoch 1 Step 1000/10125 | loss=0.5053\n",
            "Epoch 1 Step 1200/10125 | loss=0.4748\n",
            "Epoch 1 Step 1400/10125 | loss=0.4536\n",
            "Epoch 1 Step 1600/10125 | loss=0.4323\n",
            "Epoch 1 Step 1800/10125 | loss=0.4142\n",
            "Epoch 1 Step 2000/10125 | loss=0.3980\n",
            "Epoch 1 Step 2200/10125 | loss=0.3847\n",
            "Epoch 1 Step 2400/10125 | loss=0.3744\n",
            "Epoch 1 Step 2600/10125 | loss=0.3642\n",
            "Epoch 1 Step 2800/10125 | loss=0.3548\n",
            "Epoch 1 Step 3000/10125 | loss=0.3461\n",
            "Epoch 1 Step 3200/10125 | loss=0.3384\n",
            "Epoch 1 Step 3400/10125 | loss=0.3314\n",
            "Epoch 1 Step 3600/10125 | loss=0.3238\n",
            "Epoch 1 Step 3800/10125 | loss=0.3166\n",
            "Epoch 1 Step 4000/10125 | loss=0.3113\n",
            "Epoch 1 Step 4200/10125 | loss=0.3064\n",
            "Epoch 1 Step 4400/10125 | loss=0.3008\n",
            "Epoch 1 Step 4600/10125 | loss=0.2957\n",
            "Epoch 1 Step 4800/10125 | loss=0.2913\n",
            "Epoch 1 Step 5000/10125 | loss=0.2869\n",
            "Epoch 1 Step 5200/10125 | loss=0.2829\n",
            "Epoch 1 Step 5400/10125 | loss=0.2785\n",
            "Epoch 1 Step 5600/10125 | loss=0.2751\n",
            "Epoch 1 Step 5800/10125 | loss=0.2721\n",
            "Epoch 1 Step 6000/10125 | loss=0.2691\n",
            "Epoch 1 Step 6200/10125 | loss=0.2658\n",
            "Epoch 1 Step 6400/10125 | loss=0.2624\n",
            "Epoch 1 Step 6600/10125 | loss=0.2600\n",
            "Epoch 1 Step 6800/10125 | loss=0.2574\n",
            "Epoch 1 Step 7000/10125 | loss=0.2546\n",
            "Epoch 1 Step 7200/10125 | loss=0.2521\n",
            "Epoch 1 Step 7400/10125 | loss=0.2490\n",
            "Epoch 1 Step 7600/10125 | loss=0.2469\n",
            "Epoch 1 Step 7800/10125 | loss=0.2447\n",
            "Epoch 1 Step 8000/10125 | loss=0.2427\n",
            "Epoch 1 Step 8200/10125 | loss=0.2408\n",
            "Epoch 1 Step 8400/10125 | loss=0.2387\n",
            "Epoch 1 Step 8600/10125 | loss=0.2370\n",
            "Epoch 1 Step 8800/10125 | loss=0.2351\n",
            "Epoch 1 Step 9000/10125 | loss=0.2334\n",
            "Epoch 1 Step 9200/10125 | loss=0.2315\n",
            "Epoch 1 Step 9400/10125 | loss=0.2299\n",
            "Epoch 1 Step 9600/10125 | loss=0.2283\n",
            "Epoch 1 Step 9800/10125 | loss=0.2265\n",
            "Epoch 1 Step 10000/10125 | loss=0.2247\n",
            "[Val] epoch=1 | acc=0.9458 f1=0.9470 auc=0.9881 thr=0.50\n",
            "Epoch 2 Step 200/10125 | loss=0.1302\n",
            "Epoch 2 Step 400/10125 | loss=0.1293\n",
            "Epoch 2 Step 600/10125 | loss=0.1223\n",
            "Epoch 2 Step 800/10125 | loss=0.1180\n",
            "Epoch 2 Step 1000/10125 | loss=0.1184\n",
            "Epoch 2 Step 1200/10125 | loss=0.1192\n",
            "Epoch 2 Step 1400/10125 | loss=0.1199\n",
            "Epoch 2 Step 1600/10125 | loss=0.1185\n",
            "Epoch 2 Step 1800/10125 | loss=0.1184\n",
            "Epoch 2 Step 2000/10125 | loss=0.1176\n",
            "Epoch 2 Step 2200/10125 | loss=0.1168\n",
            "Epoch 2 Step 2400/10125 | loss=0.1160\n",
            "Epoch 2 Step 2600/10125 | loss=0.1156\n",
            "Epoch 2 Step 2800/10125 | loss=0.1163\n",
            "Epoch 2 Step 3000/10125 | loss=0.1161\n",
            "Epoch 2 Step 3200/10125 | loss=0.1150\n",
            "Epoch 2 Step 3400/10125 | loss=0.1158\n",
            "Epoch 2 Step 3600/10125 | loss=0.1163\n",
            "Epoch 2 Step 3800/10125 | loss=0.1153\n",
            "Epoch 2 Step 4000/10125 | loss=0.1147\n",
            "Epoch 2 Step 4200/10125 | loss=0.1141\n",
            "Epoch 2 Step 4400/10125 | loss=0.1143\n",
            "Epoch 2 Step 4600/10125 | loss=0.1137\n",
            "Epoch 2 Step 4800/10125 | loss=0.1135\n",
            "Epoch 2 Step 5000/10125 | loss=0.1134\n",
            "Epoch 2 Step 5200/10125 | loss=0.1132\n",
            "Epoch 2 Step 5400/10125 | loss=0.1127\n",
            "Epoch 2 Step 5600/10125 | loss=0.1122\n",
            "Epoch 2 Step 5800/10125 | loss=0.1120\n",
            "Epoch 2 Step 6000/10125 | loss=0.1116\n",
            "Epoch 2 Step 6200/10125 | loss=0.1110\n",
            "Epoch 2 Step 6400/10125 | loss=0.1109\n",
            "Epoch 2 Step 6600/10125 | loss=0.1107\n",
            "Epoch 2 Step 6800/10125 | loss=0.1105\n",
            "Epoch 2 Step 7000/10125 | loss=0.1101\n",
            "Epoch 2 Step 7200/10125 | loss=0.1098\n",
            "Epoch 2 Step 7400/10125 | loss=0.1097\n",
            "Epoch 2 Step 7600/10125 | loss=0.1096\n",
            "Epoch 2 Step 7800/10125 | loss=0.1094\n",
            "Epoch 2 Step 8000/10125 | loss=0.1094\n",
            "Epoch 2 Step 8200/10125 | loss=0.1093\n",
            "Epoch 2 Step 8400/10125 | loss=0.1088\n",
            "Epoch 2 Step 8600/10125 | loss=0.1084\n",
            "Epoch 2 Step 8800/10125 | loss=0.1081\n",
            "Epoch 2 Step 9000/10125 | loss=0.1077\n",
            "Epoch 2 Step 9200/10125 | loss=0.1074\n",
            "Epoch 2 Step 9400/10125 | loss=0.1072\n",
            "Epoch 2 Step 9600/10125 | loss=0.1071\n",
            "Epoch 2 Step 9800/10125 | loss=0.1067\n",
            "Epoch 2 Step 10000/10125 | loss=0.1062\n",
            "[Val] epoch=2 | acc=0.9636 f1=0.9642 auc=0.9935 thr=0.35\n",
            "Epoch 3 Step 200/10125 | loss=0.0743\n",
            "Epoch 3 Step 400/10125 | loss=0.0689\n",
            "Epoch 3 Step 600/10125 | loss=0.0678\n",
            "Epoch 3 Step 800/10125 | loss=0.0698\n",
            "Epoch 3 Step 1000/10125 | loss=0.0698\n",
            "Epoch 3 Step 1200/10125 | loss=0.0691\n",
            "Epoch 3 Step 1400/10125 | loss=0.0684\n",
            "Epoch 3 Step 1600/10125 | loss=0.0681\n",
            "Epoch 3 Step 1800/10125 | loss=0.0671\n",
            "Epoch 3 Step 2000/10125 | loss=0.0662\n",
            "Epoch 3 Step 2200/10125 | loss=0.0667\n",
            "Epoch 3 Step 2400/10125 | loss=0.0667\n",
            "Epoch 3 Step 2600/10125 | loss=0.0660\n",
            "Epoch 3 Step 2800/10125 | loss=0.0654\n",
            "Epoch 3 Step 3000/10125 | loss=0.0656\n",
            "Epoch 3 Step 3200/10125 | loss=0.0656\n",
            "Epoch 3 Step 3400/10125 | loss=0.0657\n",
            "Epoch 3 Step 3600/10125 | loss=0.0653\n",
            "Epoch 3 Step 3800/10125 | loss=0.0645\n",
            "Epoch 3 Step 4000/10125 | loss=0.0643\n",
            "Epoch 3 Step 4200/10125 | loss=0.0640\n",
            "Epoch 3 Step 4400/10125 | loss=0.0639\n",
            "Epoch 3 Step 4600/10125 | loss=0.0633\n",
            "Epoch 3 Step 4800/10125 | loss=0.0629\n",
            "Epoch 3 Step 5000/10125 | loss=0.0629\n",
            "Epoch 3 Step 5200/10125 | loss=0.0627\n",
            "Epoch 3 Step 5400/10125 | loss=0.0624\n",
            "Epoch 3 Step 5600/10125 | loss=0.0623\n",
            "Epoch 3 Step 5800/10125 | loss=0.0622\n",
            "Epoch 3 Step 6000/10125 | loss=0.0618\n",
            "Epoch 3 Step 6200/10125 | loss=0.0618\n",
            "Epoch 3 Step 6400/10125 | loss=0.0617\n",
            "Epoch 3 Step 6600/10125 | loss=0.0613\n",
            "Epoch 3 Step 6800/10125 | loss=0.0610\n",
            "Epoch 3 Step 7000/10125 | loss=0.0607\n",
            "Epoch 3 Step 7200/10125 | loss=0.0607\n",
            "Epoch 3 Step 7400/10125 | loss=0.0607\n",
            "Epoch 3 Step 7600/10125 | loss=0.0606\n",
            "Epoch 3 Step 7800/10125 | loss=0.0607\n",
            "Epoch 3 Step 8000/10125 | loss=0.0608\n",
            "Epoch 3 Step 8200/10125 | loss=0.0607\n",
            "Epoch 3 Step 8400/10125 | loss=0.0606\n",
            "Epoch 3 Step 8600/10125 | loss=0.0607\n",
            "Epoch 3 Step 8800/10125 | loss=0.0606\n",
            "Epoch 3 Step 9000/10125 | loss=0.0604\n",
            "Epoch 3 Step 9200/10125 | loss=0.0604\n",
            "Epoch 3 Step 9400/10125 | loss=0.0602\n",
            "Epoch 3 Step 9600/10125 | loss=0.0601\n",
            "Epoch 3 Step 9800/10125 | loss=0.0599\n",
            "Epoch 3 Step 10000/10125 | loss=0.0599\n",
            "[Val] epoch=3 | acc=0.9680 f1=0.9685 auc=0.9948 thr=0.20\n",
            "[Done] Best F1=0.9685, thr=0.20\n"
          ]
        }
      ],
      "source": [
        "# ===== Colab-ready: Code Similarity Cross-Encoder Fine-tuning =====\n",
        "# - CSV columns: ['code_a_norm','code_b_norm','similar'] (0/1)\n",
        "# - Pretrained backbone(.pt) 로드 → 이진 판별 헤드\n",
        "# - AMP, Scheduler, F1기반 threshold 튜닝, Best save\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "import os, math, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "# --------- [1] Paths ----------\n",
        "PRETRAINED_CKPT = \"/content/drive/MyDrive/models/rolebert/rolebert_scratch.pt\"\n",
        "TOKENIZER_FILE  = \"/content/drive/MyDrive/models/rolebert/tokenizer.json\"\n",
        "TOKENIZER_DIR   = os.path.dirname(TOKENIZER_FILE)\n",
        "TRAIN_CSV       = \"/content/drive/MyDrive/dacon_preprocess_data/train_pairs_real_final_180000.csv\"\n",
        "OUT_DIR         = \"/content/drive/MyDrive/rolebert_finetune_out\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------- [2] Basic config ----------\n",
        "SEED         = 42\n",
        "MAX_LEN      = 512\n",
        "BATCH_SIZE   = 16\n",
        "EPOCHS       = 3\n",
        "LR_BACKBONE  = 2e-5\n",
        "LR_HEAD      = 1e-4\n",
        "WARMUP_RATIO = 0.05\n",
        "VAL_RATIO    = 0.1\n",
        "GRAD_ACCUM   = 1\n",
        "WEIGHT_DECAY = 0.01\n",
        "USE_AMP      = True\n",
        "BACKBONE_SKELETON = \"bert-base-uncased\"   # 구조 스켈레톤(가중치는 .pt에서 주입)\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------- [3] Tokenizer ----------\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
        "\n",
        "# --------- [4] Dataset ----------\n",
        "REQUIRED_COLS = {\"code_a_norm\",\"code_b_norm\",\"similar\"}\n",
        "\n",
        "def load_pairs_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "    # 유연 매핑: 대소문자 혼재 대비\n",
        "    need = {}\n",
        "    for k in REQUIRED_COLS:\n",
        "        if k in cols_lower:\n",
        "            need[k] = cols_lower[k]\n",
        "        else:\n",
        "            raise ValueError(f\"CSV에 '{k}' 컬럼이 필요합니다. 현재: {list(df.columns)}\")\n",
        "    df = df.rename(columns={\n",
        "        need[\"code_a_norm\"]: \"code_a_norm\",\n",
        "        need[\"code_b_norm\"]: \"code_b_norm\",\n",
        "        need[\"similar\"]:     \"similar\",\n",
        "    })\n",
        "    df[\"similar\"] = df[\"similar\"].astype(int)\n",
        "    return df\n",
        "\n",
        "df_all = load_pairs_csv(TRAIN_CSV)\n",
        "\n",
        "# --------- [5] Split ----------\n",
        "n_total = len(df_all)\n",
        "n_val   = max(1, int(n_total * VAL_RATIO))\n",
        "n_train = n_total - n_val\n",
        "train_df, val_df = random_split(df_all, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
        "train_df = df_all.iloc[train_df.indices].reset_index(drop=True)\n",
        "val_df   = df_all.iloc[val_df.indices].reset_index(drop=True)\n",
        "\n",
        "# --------- [6] Datasets / Loaders ----------\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=512):\n",
        "        self.df, self.tk, self.max_len = df, tokenizer, max_len\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        a = str(self.df.loc[i, \"code_a_norm\"])\n",
        "        b = str(self.df.loc[i, \"code_b_norm\"])\n",
        "        y = float(self.df.loc[i, \"similar\"])\n",
        "        enc = self.tk(a, b, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        # 허용 키만 추려서 예기치 않은 키 전달 방지\n",
        "        allowed = {k: v.squeeze(0) for k, v in enc.items() if k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"]}\n",
        "        allowed[\"labels\"] = torch.tensor(y, dtype=torch.float)\n",
        "        return allowed\n",
        "\n",
        "train_ds = PairDataset(train_df, tokenizer, MAX_LEN)\n",
        "val_ds   = PairDataset(val_df, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------- [7] Model ----------\n",
        "class CrossEncoder(nn.Module):\n",
        "    def __init__(self, backbone_name_or_dir):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(backbone_name_or_dir)\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.head = nn.Linear(hidden, 1)\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls = out.last_hidden_state[:, 0]\n",
        "        logit = self.head(cls).squeeze(-1)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.BCEWithLogitsLoss()(logit, labels)\n",
        "        return {\"loss\": loss, \"logits\": logit}\n",
        "\n",
        "model = CrossEncoder(BACKBONE_SKELETON)\n",
        "# 토크나이저 크기 맞추기(필요 시)\n",
        "try:\n",
        "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --------- [8] Load pretrained checkpoint (.pt) ----------\n",
        "ckpt = torch.load(PRETRAINED_CKPT, map_location=\"cpu\")\n",
        "state_dict = ckpt.get(\"state_dict\", ckpt)\n",
        "new_sd = model.state_dict()\n",
        "loaded = 0\n",
        "for k, v in state_dict.items():\n",
        "    mk = k[7:] if k.startswith(\"module.\") else k\n",
        "    if mk in new_sd and isinstance(v, torch.Tensor) and new_sd[mk].shape == v.shape:\n",
        "        new_sd[mk] = v; loaded += 1\n",
        "model.load_state_dict(new_sd, strict=False)\n",
        "print(f\"[Checkpoint] Loaded backbone params: {loaded}\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# --------- [9] Optim / Scheduler ----------\n",
        "head_params = list(model.head.parameters())\n",
        "backbone_params = [p for n,p in model.named_parameters() if p.requires_grad and not n.startswith(\"head.\")]\n",
        "optimizer = AdamW(\n",
        "    [{\"params\": backbone_params, \"lr\": LR_BACKBONE},\n",
        "     {\"params\": head_params, \"lr\": LR_HEAD}],\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "total_steps = math.ceil(len(train_loader)/GRAD_ACCUM) * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "# --------- [10] Train & Eval ----------\n",
        "def sigmoid_np(x): return 1/(1+np.exp(-x))\n",
        "\n",
        "def evaluate(model, loader, best_thr=None):\n",
        "    model.eval()\n",
        "    probs, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            # 안전: 허용 키만 모델에 전달\n",
        "            for k in list(batch.keys()):\n",
        "                if hasattr(batch[k], \"to\"): batch[k] = batch[k].to(device)\n",
        "            inputs = {k: batch[k] for k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"] if k in batch}\n",
        "            out = model(**inputs)\n",
        "            logits = out[\"logits\"].detach().cpu().numpy()\n",
        "            probs.append(sigmoid_np(logits))\n",
        "            labels.append(batch[\"labels\"].detach().cpu().numpy())\n",
        "    probs = np.concatenate(probs)\n",
        "    labels = np.concatenate(labels).astype(int)\n",
        "    thr = 0.5 if best_thr is None else best_thr\n",
        "    if best_thr is None:\n",
        "        cand = np.linspace(0.05, 0.95, 19)\n",
        "        f1s = [f1_score(labels, (probs>=t).astype(int)) for t in cand]\n",
        "        thr = float(cand[int(np.argmax(f1s))])\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(labels, preds)),\n",
        "        \"f1\":  float(f1_score(labels, preds)),\n",
        "        \"auc\": float(roc_auc_score(labels, probs)) if len(np.unique(labels))>1 else float(\"nan\"),\n",
        "        \"thr\": float(thr),\n",
        "    }\n",
        "\n",
        "best_f1, best_thr = -1.0, 0.5\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    for step, batch in enumerate(train_loader, 1):\n",
        "        for k in list(batch.keys()):\n",
        "            if hasattr(batch[k], \"to\"): batch[k] = batch[k].to(device)\n",
        "        inputs = {k: batch[k] for k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"] if k in batch}\n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            out = model(**inputs)\n",
        "            loss = nn.BCEWithLogitsLoss()(out[\"logits\"], batch[\"labels\"])\n",
        "        scaler.scale(loss).backward()\n",
        "        if step % GRAD_ACCUM == 0:\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "        running += loss.item()\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Epoch {epoch} Step {step}/{len(train_loader)} | loss={running/step:.4f}\")\n",
        "\n",
        "    val_metrics = evaluate(model, val_loader, best_thr=None)\n",
        "    print(f\"[Val] epoch={epoch} | acc={val_metrics['acc']:.4f} f1={val_metrics['f1']:.4f} auc={val_metrics['auc']:.4f} thr={val_metrics['thr']:.2f}\")\n",
        "\n",
        "    if val_metrics[\"f1\"] > best_f1:\n",
        "        best_f1, best_thr = val_metrics[\"f1\"], val_metrics[\"thr\"]\n",
        "        best_state = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"config\": {\"max_len\": MAX_LEN, \"thr\": best_thr, \"backbone_skeleton\": BACKBONE_SKELETON},\n",
        "        }\n",
        "        torch.save(best_state, os.path.join(OUT_DIR, \"best_crossencoder.pt\"))\n",
        "        # 편의용 HF 저장\n",
        "        try:\n",
        "            model.backbone.save_pretrained(os.path.join(OUT_DIR, \"hf_backbone\"))\n",
        "            tokenizer.save_pretrained(os.path.join(OUT_DIR, \"hf_tokenizer\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(f\"[Done] Best F1={best_f1:.4f}, thr={best_thr:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Paths ---------\n",
        "BEST_PT        = \"/content/drive/MyDrive/rolebert_finetune_out/best_crossencoder.pt\"  # fine tuning best\n",
        "TOKENIZER_DIR  = \"/content/drive/MyDrive/models/rolebert\"\n",
        "TEST_CSV       = \"/content/drive/MyDrive/dacon_preprocess_data/test_norm_final.csv\"\n",
        "OUT_DIR        = \"/content/drive/MyDrive/rolebert_finetune_out\"\n",
        "OUT_CSV        = os.path.join(OUT_DIR, \"preds_test.csv\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------- Model skeleton (학습 때와 동일) ---------\n",
        "BACKBONE_SKELETON = \"bert-base-uncased\"\n",
        "\n",
        "class CrossEncoder(nn.Module):\n",
        "    def __init__(self, backbone_name_or_dir):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(backbone_name_or_dir)\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.head = nn.Linear(hidden, 1)\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls = out.last_hidden_state[:, 0]\n",
        "        logit = self.head(cls).squeeze(-1)\n",
        "        return logit  # (B,)\n",
        "\n",
        "# --------- Load tokenizer ---------\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
        "\n",
        "# --------- Rebuild model & load checkpoint ---------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CrossEncoder(BACKBONE_SKELETON)\n",
        "try:\n",
        "    model.backbone.resize_token_embeddings(len(tokenizer))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "ckpt = torch.load(BEST_PT, map_location=\"cpu\")\n",
        "state_dict = ckpt.get(\"model\", ckpt)  # {'model': state_dict, 'config': {...}} 형태 대비\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"[Load] missing:\", missing, \"| unexpected:\", unexpected)\n",
        "\n",
        "cfg = ckpt.get(\"config\", {})\n",
        "MAX_LEN = int(cfg.get(\"max_len\", 512))\n",
        "BEST_THR = float(cfg.get(\"thr\", 0.5))\n",
        "print(f\"[Config] max_len={MAX_LEN}, threshold={BEST_THR:.3f}\")\n",
        "\n",
        "model.to(device).eval()\n",
        "\n",
        "# --------- Load test CSV ---------\n",
        "def load_test(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    need = {}\n",
        "    for k in [\"pair_id\",\"code1_norm\",\"code2_norm\"]:\n",
        "        if k in cols: need[k] = cols[k]\n",
        "        else: raise ValueError(f\"테스트 CSV에 '{k}' 컬럼이 필요합니다. 현재 컬럼: {list(df.columns)}\")\n",
        "    df = df.rename(columns={\n",
        "        need[\"pair_id\"]: \"pair_id\",\n",
        "        need[\"code1_norm\"]: \"code1_norm\",\n",
        "        need[\"code2_norm\"]: \"code2_norm\",\n",
        "    })\n",
        "    return df\n",
        "\n",
        "test_df = load_test(TEST_CSV)\n",
        "\n",
        "# --------- Dataset / DataLoader ---------\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=512):\n",
        "        self.df, self.tk, self.max_len = df, tokenizer, max_len\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        a = str(self.df.loc[i, \"code1_norm\"])\n",
        "        b = str(self.df.loc[i, \"code2_norm\"])\n",
        "        enc = self.tk(a, b, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        # 허용 키만 추출\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items() if k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"]}\n",
        "        item[\"pair_id\"] = str(self.df.loc[i, \"pair_id\"])\n",
        "        return item\n",
        "\n",
        "test_ds = TestDataset(test_df, tokenizer, MAX_LEN)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------- Inference ---------\n",
        "all_pair_id, all_prob = [], []\n",
        "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        pair_ids = batch.pop(\"pair_id\")\n",
        "        # to(device)\n",
        "        for k in list(batch.keys()):\n",
        "            if hasattr(batch[k], \"to\"):\n",
        "                batch[k] = batch[k].to(device)\n",
        "        # 모델 입력 (허용키만)\n",
        "        inputs = {k: batch[k] for k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"] if k in batch}\n",
        "        logits = model(**inputs)              # (B,)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        all_pair_id.extend(pair_ids)\n",
        "        all_prob.extend(probs.tolist())\n",
        "\n",
        "# --------- Threshold → 0/1 ---------\n",
        "preds = (np.array(all_prob) >= BEST_THR).astype(int)\n",
        "\n",
        "# --------- Save CSV: pair_id, similar ---------\n",
        "out_df = pd.DataFrame({\"pair_id\": all_pair_id, \"similar\": preds})\n",
        "out_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "print(f\"[Saved] {OUT_CSV}  rows={len(out_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v535D3etEmm",
        "outputId": "5c164c39-50c5-4d5e-cb30-11a0b9f0ee80"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] missing: [] | unexpected: []\n",
            "[Config] max_len=512, threshold=0.200\n",
            "[Saved] /content/drive/MyDrive/rolebert_finetune_out/preds_test.csv  rows=179700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8qGLOvWH78x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}