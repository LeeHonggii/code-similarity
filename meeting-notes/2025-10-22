# 회의록 - 2025.10.22

## 📅 회의 정보
- **날짜**: 2025년 10월 22일
- **참석자**: 이서율, 조병률, 황호성, 오정탁, 이홍기
- **주제**: 최종 모델 학습 결과 공유 및 프로젝트 완료

---

## 📋 안건

### 1. 각자 모델 학습 결과 공유

#### 모델별 성능 결과

| 순위 | 팀원 | 모델 | Public Score | Private Score | 특징 |
|------|------|------|--------------|---------------|------|
| 🥇 | **이홍기** | CodeBERT (HF) | **92.84%** | **92.86%** | microsoft/codebert-base 활용 |
| 🥈 | **황호성** | RoBERTa-small | **91.67%** | **91.64%** | 전처리 최적화 + Unigram |
| 🥉 | **조병률** | Custom BERT | **86.33%** | **86.32%** | 기본 MLM, Baseline |
| 4 | **오정탁** | RoleBERT | - | **49%** | Role Embedding 시도 |
| 5 | **이서율** | Contrastive | **62.18%** | **62.03%** | 대조학습 + AST |

---

## 💬 각 팀원 발표 내용

### 이홍기 - CodeBERT (HuggingFace) 🥇

**접근 방법**
- HuggingFace의 사전학습된 `microsoft/codebert-base` 활용
- MLM + RTD (Replaced Token Detection) 조합

**주요 전략**
- Transfer Learning 최대 활용
- Full Fine-tuning 방식
- AdamW + Linear Warmup Scheduler

**결과**
- **Public**: 92.84%
- **Private**: 92.86%
- 🎉 **최고 성능 달성!**

**인사이트**
- 대규모 사전학습 모델의 효과가 압도적
- 복잡한 커스텀 구조보다 검증된 모델이 안정적

---

### 황호성 - RoBERTa-small 🥈

**접근 방법**
- Custom RoBERTa 구현
- Unigram Tokenizer (32k vocab)
- 전처리 최적화에 집중

**주요 전략**
- **좌측 절단** (Left Truncation): 코드 끝부분 우선
- 특수 토큰 설계: `<str>`, `<num>`, `<indent>`, `<dedent>`
- 최소한의 정제로 정보 보존

**결과**
- **Public**: 91.67%
- **Private**: 91.64%
- 🎉 **Custom 모델 중 최고 성능!**

**인사이트**
- 전처리 방법 개선으로 **+4.5%p** 향상
- 과도한 정제보다 적절한 최소화가 효과적
- Unigram이 희귀 식별자 처리에 안정적

---

### 조병률 - Custom BERT 🥉

**접근 방법**
- BERT 아키텍처를 처음부터 구현
- SentencePiece Tokenizer (32k vocab)
- 기본 MLM 학습

**주요 전략**
- 기본에 충실한 구현
- 표준 하이퍼파라미터 사용
- Baseline 역할

**결과**
- **Public**: 86.33%
- **Private**: 86.32%

**인사이트**
- Baseline으로서 의미 있는 성능
- 추가 최적화 여지 확인
- 확장 가능한 구조 제공

---

### 오정탁 - RoleBERT

**접근 방법**
- CodeBERT + Role Embedding
- 구문 정보(Role)를 임베딩에 명시적 추가
- AST 기반 코드 정규화

**주요 전략**
- **Role 태깅**: KEYWORD, IDENTIFIER, OP, LITERAL, SEP
- MLM + Role Prediction 멀티태스크
- Black 포매터로 코드 표준화
- SHA1 해시 기반 중복 제거 (604,124개 제거)

**결과**
- **Private**: 49%

**인사이트**
- 혁신적 시도: 구문 정보의 명시적 활용
- Role 태깅 방식 개선 필요 (휴리스틱 → 학습 기반)
- 완전 자립형 모델 구축 경험

---

### 이서율 - Contrastive Learning

**접근 방법**
- MoCo(Momentum Contrast) 기반 듀얼 인코더
- Text Encoder + AST-GNN 융합
- 대조학습 패러다임

**주요 전략**
- **Positive Pair**: 동일 문제 AC 코드
- **Hard Negative**: AC vs WA (미묘한 차이 학습)
- **Semi-hard Negative**: 유사 구조, 다른 문제
- AST 구조 정보를 GraphSAGE로 처리

**결과**
- **Public**: 62.18%
- **Private**: 62.03%

**인사이트**
- CV 기법의 코드 도메인 적응 어려움
- AST 구조 정보 활용 가능성 제시
- 구현 복잡도 대비 성능 제약 확인

---

## 🔍 종합 분석 및 토론

### 1. 사전학습 모델의 중요성
**합의 사항**
- HuggingFace 사전학습 모델 >> Custom 모델
- 대규모 데이터로 학습된 모델의 압도적 효과
- Transfer Learning이 핵심 전략

**데이터**
- 이홍기(HF 모델): 92.86%
- 황호성(Custom 최적화): 91.64%
- 조병률(Custom 기본): 86.32%

### 2. 전처리 전략의 영향
**황호성의 전처리 효과**
- 방법 A (좌측 절단 + 최소화): 91.67%
- 방법 B: 87.13%
- **차이**: +4.54%p

**교훈**
- 전처리 품질 > 데이터 크기
- 코드 끝부분(로직 핵심)이 중요
- 과도한 정제는 정보 손실 유발

### 3. 복잡도 vs 성능
**발견**
- 단순 구조 + 최적화 > 복잡한 구조
- 이서율의 듀얼 인코더: 구현 복잡, 성능 제약
- 황호성의 단순 RoBERTa: 효율적, 높은 성능

### 4. 구조 정보 활용
**시도들**
- 이서율: AST-GNN (명시적 구조 학습)
- 오정탁: Role Embedding (구문 정보)
- 결과: 가능성은 있으나 추가 최적화 필요

### 5. Tokenizer 선택
**비교**
- Unigram (황호성): 희귀 식별자 안정적, 소규모 데이터 적합
- BPE (조병률, 오정탁, 이홍기): 표준적, 안정적
- SentencePiece (조병률): 범용적

---

## 💡 핵심 인사이트

### 성공 요인
1. **대규모 사전학습 모델** (이홍기)
   - Transfer Learning의 위력
   - 검증된 모델의 안정성

2. **효과적인 전처리** (황호성)
   - 좌측 절단 전략
   - 적절한 정보 보존

3. **안정적 학습 전략** (공통)
   - AdamW + Warmup Scheduler
   - Full Fine-tuning

### 실험의 가치
1. **혁신적 시도** (오정탁, 이서율)
   - Role Embedding: 구문 정보 활용 가능성
   - Contrastive Learning: 대조학습 패러다임
   - 실패에서 배우는 교훈

2. **Baseline 구축** (조병률)
   - 비교 기준 제공
   - 개선 방향 명확화

---

## 📊 최종 순위 정리

### 데이콘 리더보드

| 순위 | 이름 | 모델 | Score | 접근법 |
|------|------|------|-------|--------|
| 🥇 | 이홍기 | CodeBERT (HF) | **92.86%** | HF 사전학습 모델 |
| 🥈 | 황호성 | RoBERTa-small | **91.64%** | 전처리 최적화 |
| 🥉 | 조병률 | Custom BERT | **86.32%** | 기본 MLM |
| 4 | 이서율 | Contrastive | 62.03% | 대조학습 + AST |
| 5 | 오정탁 | RoleBERT | 49% | Role Embedding |

---

## 📝 향후 개선 방향

### 공통 개선안
1. **앙상블**
   - 여러 모델 조합
   - Soft/Hard voting
   - 성능 추가 향상 기대

2. **데이터 증강**
   - 더 많은 CodeNet 데이터
   - Back-translation
   - 다양한 언어 포함

3. **구조 정보 경량 통합**
   - AST 정보를 보조적으로
   - Role Embedding 개선

### 개별 개선안

**이홍기**
- 앙상블 시도
- 전처리 최적화 (황호성 방식)

**황호성**
- HF 모델 시도 (벤치마크)
- 구조 정보 통합

**조병률**
- RoBERTa 기법 적용
- Dynamic masking

**오정탁**
- Role 태깅 개선 (학습 기반)
- 성능 검증 및 분석

**이서율**
- 코드 도메인 특화 대조학습
- 경량화 및 안정화

---

## ✅ 다음 액션 아이템

### 문서화
- [x] 최종 결과 정리
- [x] 종합 비교 분석 문서 작성 (`5명_종합비교분석.md`)
- [x] README 업데이트
- [x] 회의록 정리

### 프로젝트 마무리
- [x] 코드 정리 및 주석 추가
- [x] 모델 파일 정리 (`/models`)
- [x] 토크나이저 파일 정리 (`/tokenizers`)
- [x] 학습 스크립트 정리 (`/train`)

---

## 🎓 프로젝트 회고

### 팀 시너지
**5가지 다른 접근법의 가치**
- **기초**: 조병률의 Baseline
- **최적화**: 황호성의 전처리 전략
- **혁신**: 이서율, 오정탁의 실험적 시도
- **실용**: 이홍기의 HF 모델 활용

→ **다양한 시도를 통해 풍부한 인사이트 확보**

### 배운 점
1. **Transfer Learning의 중요성**
   - 검증된 사전학습 모델의 위력
   - 처음부터 만들기보다 활용하기

2. **전처리의 핵심성**
   - 데이터 품질이 성능을 좌우
   - 도메인 특성 이해 필요

3. **실험의 가치**
   - 실패도 중요한 학습
   - 다양한 접근법 시도의 의미

### 아쉬운 점
1. 앙상블 시도 부족
2. 하이퍼파라미터 튜닝 시간 부족
3. 더 많은 데이터 활용 못함

### 만족스러운 점
1. 5가지 다른 접근법 모두 완성
2. 체계적인 비교 분석
3. 팀워크와 지식 공유

---

## 🎉 프로젝트 완료

**기간**: 2025.10.17 ~ 2025.10.22 (6일)

**성과**
- ✅ 5가지 모델 구현 완료
- ✅ 최고 성능 92.86% 달성
- ✅ 종합 분석 문서 완성
- ✅ 팀 전체 학습 및 성장

**감사 인사**
팀원 모두 고생하셨습니다! 🎊

---

**회의록 작성자**: 팀 전체  
**작성일**: 2025.10.22