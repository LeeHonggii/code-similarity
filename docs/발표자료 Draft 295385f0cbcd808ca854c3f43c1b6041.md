# ë°œí‘œìë£Œ Draft

ë³´ì¡° ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ pretrained ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµ ë°ì´í„°ë¡œ fine-tuningì„ ì§„í–‰í•˜ì—¬ pretrained ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ì‹­ì‹œì˜¤.

### í•™ìŠµ ë°ì´í„°

Project CodeNet[[ë§í¬]](https://github.com/IBM/Project_CodeNet?tab=readme-ov-file)

### ë³´ì¡° ë°ì´í„°

ì›”ê°„ ë°ì´ì½˜ ì½”ë“œ ìœ ì‚¬ì„± íŒë‹¨ AI ê²½ì§„ëŒ€íšŒ[[ë§í¬]](https://dacon.io/competitions/official/235900/overview/description)

[CodeNet_EDA](https://www.notion.so/CodeNet_EDA-291385f0cbcd80b48e34fb064215b7dc?pvs=21) 

# CodeNet ì „ì²˜ë¦¬

### Ideas

<aside>
ğŸ’¡

ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ì½”ë“œëŠ” ì–´ë–»ê²Œ êµ¬ë¶„í•  ê²ƒì¸ê°€?

ì½”ë“œ ì¼ê´€ì„±ì„ ì–´ë–»ê²Œ ìœ ì§€í•  ê²ƒì¸ê°€?

ì£¼ì„ì™€ docstringì€ ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ê°€?

ì¤‘ë³µëœ ì½”ë“œë¥¼ ì–´ë–»ê²Œ ê°ì§€í•˜ê³  ì²˜ë¦¬í•  ê²ƒì¸ê°€?

ìŠ¤í˜ì…œ í† í°ì€ ì–¼ë§ˆë‚˜ ë§Œë“¤ ê²ƒì¸ê°€?

ë©”íƒ€ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì´ìš©í•  ê²ƒì¸ê°€?

</aside>

### Brainstorming

<aside>
ğŸ’¡

AST íŒŒì„œë¥¼ í†µê³¼í•˜ëŠ” ì½”ë“œë§Œ ì‚¬ìš©

python black ì½”ë“œ í¬ë§¤í„° ì‚¬ìš©

ì£¼ì„/docstring ì œê±° ë˜ëŠ” inputìœ¼ë¡œ ì‚¬ìš©

ASTíŠ¸ë¦¬ë¥¼ í†µí•´ ë™ì¼ ë¬¸ì œì—ì„œ ë™ì¼ êµ¬ì¡°ì¸ ì½”ë“œë“¤ì€ ì¤‘ë³µëœ ê²ƒìœ¼ë¡œ íŒë³„

ìŠ¤í˜ì…œí† í° <NEWLINE> <INDENT> <DEDENT>ë“± ì¶”ê°€

ì½”ë“œ ë‚´ ìˆ«ì ë³€ìˆ˜ì™€ ë¬¸ìì—´ ë³€ìˆ˜(Literal)ëŠ” <NUM> <STR> ë“±ìœ¼ë¡œ ì¹˜í™˜

ë™ì¼ ì½”ë“œ ë‚´ ë™ì¼ ë³€ìˆ˜ëŠ” ê°™ì€ ì´ë¦„ìœ¼ë¡œ ëŒ€ì²´ (var_1, var_2 â€¦)

ë©”íƒ€ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , Status: Acceptedë§Œ ì‚¬ìš©

ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ Positive/Negative Pairingì„ í†µí•´ ëŒ€ì¡° í•™ìŠµ

</aside>

### ì „ì²˜ë¦¬ test

ì½”ë“œí¬ë§·íŒ… - ì£¼ì„ì œê±° -ASTíŒŒì‹± í›„ ìŠ¤ì½”í”„ ì¼ê´€ ì¹˜í™˜ â†’ ì‹ë³„ì ì •ê·œí™” â†’ ë¦¬í„°ëŸ´ ê°’ ì •ê·œí™”

### ì „ì²˜ë¦¬ sample

```python
--- sample (no_comments) ---
 def makelist(n, m):
    return [[0 for _ in range(m)] for _ in range(n)]

N, M = map(int, input().split())
A = [0] + list(map(int, input().split()))

imos = [0]*(N+1)
for i in range(1, N+1):
    imos[i] = (A[i] + imos[i-1]) % M

d = {}
for i in range(N+1):
    if imos[i] not in d:
        d[imos[i]] = 1
    else:
        d[imos[i]] += 1

ans = 0
for v in d.values():
    ans += (v * (v-1)) // 2

print(ans)

--- sample (anon) ---
 def func_1(var_1, var_2):
    return [[0 for var_3 in range(var_2)] for var_3 in range(var_1)]
var_7, var_8 = map(int, input().split())
var_9 = [0] + list(map(int, input().split()))
var_10 = [0] * (var_7 + 1)
for var_12 in range(1, var_7 + 1):
    var_10[var_12] = (var_9[var_12] + var_10[var_12 - 1]) % var_8
var_21 = {}
for var_12 in range(var_7 + 1):
    if var_10[var_12] not in var_21:
        var_21[var_10[var_12]] = 1
    else:
        var_21[var_10[var_12]] += 1
var_33 = 0
for var_34 in var_21.values():
    var_33 += var_34 * (var_34 - 1) // 2
print(var_33)

--- sample (norm) ---
 def func_1(var_1, var_2):
    return [[<NUM_D1> for var_3 in range(var_2)] for var_3 in range(var_1)]
var_7, var_8 = map(int, input().split())
var_9 = [<NUM_D1>] + list(map(int, input().split()))
var_10 = [<NUM_D1>] * (var_7 + <NUM_D1>)
for var_12 in range(<NUM_D1>, var_7 + <NUM_D1>):
    var_10[var_12] = (var_9[var_12] + var_10[var_12 - <NUM_D1>]) % var_8
var_21 = {}
for var_12 in range(var_7 + <NUM_D1>):
    if var_10[var_12] not in var_21:
        var_21[var_10[var_12]] = <NUM_D1>
    else:
        var_21[var_10[var_12]] += <NUM_D1>
var_33 = <NUM_D1>
for var_34 in var_21.values():
    var_33 += var_34 * (var_34 - <NUM_D1>) // <NUM_D1>
print(var_33)

--- sample (with_layout) ---
 def func_1 ( var_1 , var_2 ) : <NL> <INDENT> return [ [ < NUM_D1 > for var_3 in range ( var_2 ) ] for var_3 in range ( var_1 ) ] <NL> <DEDENT> var_7 , var_8 = map ( int , input ( ) . split ( ) ) <NL> var_9 = [ < NUM_D1 > ] + list ( map ( int , input ( ) . split ( ) ) ) <NL> var_10 = [ < NUM_D1 > ] * ( var_7 + < NUM_D1 > ) <NL> for var_12 in range ( < NUM_D1 > , var_7 + < NUM_D1 > ) : <NL> <INDENT> var_10 [ var_12 ] = ( var_9 [ var_12 ] + var_10 [ var_12 - < NUM_D1 > ] ) % var_8 <NL> <DEDENT> var_21 = { } <NL> for var_12 in range ( var_7 + < NUM_D1 > ) : <NL> <INDENT> if var_10 [ var_12 ] not in var_21 : <NL> <INDENT> var_21 [ var_10 [ var_12 ] ] = < NUM_D1 > <NL> <DEDENT> else : <NL> <INDENT> var_21 [ var_10 [ var_12 ] ] += < NUM_D1 > <NL> <DEDENT> <DEDENT> var_33 = < NUM_D1 > <NL> for var_34 in var_21 . values ( ) : <NL> <INDENT> var_33 += var_34 * ( var_34 - < NUM_D1 > ) // < NUM_D1 > <NL> <DEDENT> print ( var_33 ) <NL> 

```

### ë¬¸ì œ

1. Black formatterëŠ” ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼ â†’ ì •ê·œì‹ì„ ì´ìš©í•´ ë“¤ì—¬ì“°ê¸°, ê³µë°±ë§Œ í¬ë§¤íŒ…
2. ASTíŠ¸ë¦¬ë¥¼ ì´ìš©í•œ ì¤‘ë³µì œê±°ëŠ” í¬ë§·íŒ…ì´ í¬ê²Œ í•„ìš”í•˜ì§€ ì•ŠìŒ â†’ ìœ ì§€
3. ì½”ë“œ ë‚´ ê°œí–‰ê³¼ ë“¤ì—¬ì“°ê¸° ì˜ë¯¸ ë³´ì¡´ì„ ìœ„í•´ <NEWLINE><INDENT><DEDENT>ë“±ì˜ ìŠ¤í˜ì…œí† í°ì„ ì‚¬ìš©í•˜ê³ ì í–ˆìœ¼ë‚˜, ë¦¬ì„œì¹˜ë¥¼ í†µí•´ code-specific BPE ë“±ì„ í†µí•´ ì˜ë¯¸ ë³´ì¡´ì´ ê°€ëŠ¥í•œ ê²ƒì„ í™•ì¸í•¨ â†’ì‚¬ìš© ì¤‘ì§€

### ìµœì¢… ì „ì²˜ë¦¬ ê³¼ì •

1. ì£¼ì„ ì œê±° â†’ ê³µë°±, ë“¤ì—¬ì“°ê¸° ì •ê·œí™” â†’ AST íŒŒì„œ ê²€ì¦ â†’ AST ê¸°ë°˜ ì¤‘ë³µ ì½”ë“œ ì œê±°
2. ì£¼ì„ ì œê±° â†’ ì‹ë³„ì ìµëª…í™” â†’ ê°œí–‰ ì •ë¦¬ â†’ Black í¬ë§· â†’ SHA1 í•´ì‹œ â†’ ì¤‘ë³µ ì œê±°

# Code-specific LM Research

## ëª¨ë¸ ê°œìš” ë¹„êµ

| êµ¬ë¶„ | RoBERTa-small | ELECTRA-small | CodeT5-small |
| --- | --- | --- | --- |
| **ì•„í‚¤í…ì²˜** | Encoder-only | Encoder-only (Gen + Disc) | Encoder-Decoder |
| **í¬ê¸°** | 6L Ã— H512 (50-60M) | Gen: 2-3L Ã— H256<br>Disc: 6L Ã— H512 | Encoder + Decoder (ì†Œí˜•) |
| **í•™ìŠµ ë°©ì‹** | MLM (Masked LM) | RTD (Replaced Token Detection) | Span Denoising |
| **ìµœì  ìš©ë„** | ì„ë² ë”© ë¹„êµ, ìœ ì‚¬ë„/ê²€ìƒ‰, í´ë¡  íƒì§€ | ë°ì´í„° íš¨ìœ¨ ê·¹ëŒ€í™”, ì†Œê·œëª¨ ë°ì´í„° í•™ìŠµ | ì´í•´+ìƒì„± ê²¸ìš©, ìš”ì•½/ì£¼ì„/ë³µì› |

## í•™ìŠµ íŠ¹ì„± ë¹„êµ

| êµ¬ë¶„ | RoBERTa-small | ELECTRA-small | CodeT5-small |
| --- | --- | --- | --- |
| **í•™ìŠµ ì‹ í˜¸** | ë§ˆìŠ¤í‚¹ëœ 15% ìœ„ì¹˜ë§Œ | ëª¨ë“  í† í° ìœ„ì¹˜ (ì§„ì§œ/ê°€ì§œ íŒë³„) | ì—°ì† span ë³µì› |
| **ìƒ˜í”Œ íš¨ìœ¨** | ë³´í†µ | ìµœê³  | ë³´í†µ-ë†’ìŒ |
| **ìˆ˜ë ´ ì†ë„** | ë¹ ë¦„ | ê°€ì¥ ë¹ ë¦„ | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ |
| **êµ¬í˜„ ë³µì¡ë„** | ë‹¨ìˆœ | ì¤‘ê°„ (2ê°œ ë„¤íŠ¸ì›Œí¬) | ë³µì¡ (Enc-Dec) |
| **ë©”ëª¨ë¦¬/ì†ë„** | ê°€ì¥ íš¨ìœ¨ì  | íš¨ìœ¨ì  | ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŒ |

## í† í¬ë‚˜ì´ì €

| êµ¬ë¶„ |  |  |  |
| --- | --- | --- | --- |
| **íƒ€ì…** | SentencePiece Unigram (32k) | Code-Specific BPE(32k) | Wordpiece(32k) |
| **ìŠ¤í˜ì…œ í† í°** | `<indent>` `<dedent>` `<newline>` `<str>` `<num>` `<pad>` `<s>` `</s>` `<mask>` `<nl>` `<py>` `<unk>` | `<pad>` `<s>` `</s>` `<mask><unk>` | `<pad>` `<s>` `</s>` `<mask><unk>` |
| **ì„ íƒ ì´ìœ ** | ì½”ë“œì˜ í¬ì†Œì„±ì— ê°•í•¨, ì†Œê·œëª¨ ë°ì´í„° ìˆ˜ë ´ ì•ˆì •ì , OOV ì–µì œ | ì—¬ëŸ¬ LMì— ì‚¬ìš©ë¨. ì½”ë“œì˜ ë“¤ì—¬ì“°ê¸°ì™€ ê°œí–‰ì„ í¬í•¨ |  |

## í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (T4 ê¸°ì¤€)

| êµ¬ë¶„ | RoBERTa-small | ELECTRA-small | CodeT5-small |
| --- | --- | --- | --- |
| **max_len** | 512 | 512 | 512 |
| **batch_size** | 32-48 | 32-48 | ì‘ê²Œ ì¡°ì • í•„ìš” |
| **epochs** | 2-3 | 2-3 | 2-3 |
| **learning_rate** | 3e-4 ~ 8e-4 | ìœ ì‚¬ | 2e-4 ~ 6e-4 |
| **warmup_ratio** | 0.05-0.1 | 0.05-0.1 | 0.05-0.1 |
| **ë§ˆìŠ¤í‚¹ ë¹„ìœ¨** | 15% | 15% (Gen) | 15-30% (span) |

## 400MB ë°ì´í„° ì í•©ì„±

| ëª¨ë¸ | 400MB ì„±ëŠ¥ | ì´ìœ  |
| --- | --- | --- |
| **RoBERTa-small** | â­â­â­â­â­ | ê°€ì¥ ì•ˆì •ì , ë¹ ë¥¸ ìˆ˜ë ´, ê²€ì¦ëœ ì„±ëŠ¥ |
| **ELECTRA-small** | â­â­â­â­â­ | ìµœê³  ìƒ˜í”Œ íš¨ìœ¨, ì†Œê·œëª¨ ë°ì´í„°ì— ìµœì  |
| **CodeT5-small** | â­â­â­â­ | ê°€ëŠ¥í•˜ë‚˜ ìˆ˜ë ´ ì‹œê°„/ì•ˆì •ì„± ì£¼ì˜ í•„ìš” |

## ì¥ë‹¨ì  ìš”ì•½

### RoBERTa-small

**ì¥ì **

- ìƒ˜í”Œ íš¨ìœ¨ ë†’ìŒ
- ì„ë² ë”© ì¶”ì¶œ ì†ë„ ë¹ ë¦„
- êµ¬í˜„ ê°„ë‹¨, T4 ì¹œí™”ì 
- ê²€ì¦ëœ ì•ˆì •ì„±

**ë‹¨ì **

- ìƒì„± ì‘ì—… ë¶ˆê°€
- ë””ì½”ë”í˜• íƒœìŠ¤í¬ ì•½í•¨

### ELECTRA-small

**ì¥ì **

- ìƒ˜í”Œ íš¨ìœ¨ ìµœê³ 
- ì†Œê·œëª¨ ë°ì´í„°ì— ìµœì 
- ë¹ ë¥¸ ì„±ëŠ¥ ë„ë‹¬
- ì„ë² ë”© í’ˆì§ˆ ìš°ìˆ˜

**ë‹¨ì **

- êµ¬í˜„ ë³µì¡ë„ ì¦ê°€
- ë‘ ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬ í•„ìš”
- íŠœë‹ íŒŒë¼ë¯¸í„° ë§ìŒ

### CodeT5-small

**ì¥ì **

- ì´í•´+ìƒì„± ë©€í‹°íƒœìŠ¤í¬
- í™•ì¥ì„± ìµœê³  (ìš”ì•½/ì£¼ì„/ë³€í™˜)
- êµ¬ì¡°ì  ë¬¸ë§¥ í•™ìŠµ ìš°ìˆ˜
- ê²°ì† ë¸”ë¡ ë³µì›ì— ê°•í•¨

**ë‹¨ì **

- ë©”ëª¨ë¦¬/ì‹œê°„ ì¦ê°€
- ìˆ˜ë ´ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼
- ë³µì¡í•œ ì•„í‚¤í…ì²˜

## ì„ íƒ ê°€ì´ë“œ

```
â”Œâ”€ ì†ë„Â·ì•ˆì •ì„± ìµœìš°ì„  â†’ RoBERTa-small
â”‚
â”œâ”€ ë°ì´í„° íš¨ìœ¨ ê·¹ëŒ€í™” â†’ ELECTRA-small
â”‚
â””â”€ ì´í•´+ìƒì„± ê²¸ìš© â†’ CodeT5-small

```

# ì½”ë“œ ìœ ì‚¬ë„ íŒë³„ - 5ëª… ì¢…í•© ë¹„êµ ë¶„ì„

## ğŸ“‹ Overview

| êµ¬ë¶„ | ì´ì„œìœ¨ | ì¡°ë³‘ë¥  | í™©í˜¸ì„± | ì˜¤ì •íƒ | ì´í™ê¸° |
| --- | --- | --- | --- | --- | --- |
| **Pretraining ë°©ì‹** | ëŒ€ì¡°í•™ìŠµ (Contrastive Learning) | Custom BERT (MLM), Custom ELECTRA | RoBERTa-small (MLM) | Custom CodeBERT (MLM + Role) | CodeBERT (MLM + RTD) |
| **í•µì‹¬ ì•„ì´ë””ì–´** | MoCo ê¸°ë°˜ ë“€ì–¼ ì¸ì½”ë” | BERT from scratch | RoBERTa ìµœì í™” êµ¬í˜„ | Role Embedding ì¶”ê°€ | RTD ë³´ì¡° íƒœìŠ¤í¬ |
| **êµ¬ì¡°ì  íŠ¹ì§•** | Text + AST ìœµí•© | ë‹¨ì¼ Transformer | ë‹¨ì¼ Transformer | ë‹¨ì¼ Transformer + Role | ë‹¨ì¼ Transformer |
| **í•™ìŠµ ëª©í‘œ** | ì˜ë¯¸ì  ìœ ì‚¬ë„ í•™ìŠµ | í† í° ë³µì› í•™ìŠµ, RTD | í† í° ë³µì› í•™ìŠµ | MLM + Role ì˜ˆì¸¡ | MLM + RTD |
| **Tokenizer** | Unigram (32k) | Code-Specific BPEw (32k) | Unigram (32k) | BPE (50k) | BPE (50k) |

---

## ì„±ëŠ¥ ë¹„êµ

### ğŸ“Š ìµœì¢… ì ìˆ˜

| ì´ë¦„ | ì ‘ê·¼ë²• | Public Score | Private Score | íŠ¹ì§• |
| --- | --- | --- | --- | --- |
| **í™©í˜¸ì„±** | RoBERTa | **0.9167** | **0.9164** | ì „ì²˜ë¦¬ ìµœì í™” |
| **ì´í™ê¸°** | MLM + RTD | **0.9284** | **0.9286** | HF ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš© |
| **ì¡°ë³‘ë¥ ** | Custom BERT | **0.8633** | **0.8632** | ê¸°ë³¸ MLM |
| **ì˜¤ì •íƒ** | Role Embedding | **0.9317** | **0.9318** | êµ¬ë¬¸ ì •ë³´ í™œìš© |
| **ì´ì„œìœ¨** | ëŒ€ì¡°í•™ìŠµ | **0.6218** | **0.6203** | AST êµ¬ì¡° í™œìš© |
| ì¡°ë³‘ë¥  | CustomE |  |  |  |

---

## 1ï¸âƒ£ ì´ì„œìœ¨ - ëŒ€ì¡°í•™ìŠµ (Contrastive Learning)

### ğŸ¯ í•µì‹¬ ì»¨ì…‰

MoCo(Momentum Contrast)ë¥¼ ì½”ë“œ ë„ë©”ì¸ì— ì ìš©í•œ **ë“€ì–¼ ì¸ì½”ë” êµ¬ì¡°**

### ğŸ“¦ í•™ìŠµ ë°ì´í„° êµ¬ì„±

**Pair ìƒì„± ì „ëµ**

| ìŒ ì¢…ë¥˜ | ì„¤ëª… | í•™ìŠµ ëª©í‘œ | ê°€ì¤‘ì¹˜ |
| --- | --- | --- | --- |
| **Positive** | ë™ì¼ ë¬¸ì œ AC ìŒ | ìœ ì‚¬(1) í•™ìŠµ | 1.0 |
| **Hard Negative** | ë™ì¼ ë¬¸ì œ AC vs WA | ë¯¸ë¬˜í•œ ì°¨ì´ êµ¬ë¶„ | 1.5 |
| **Semi-hard Negative** | ë‹¤ë¥¸ ë¬¸ì œ, ìœ ì‚¬ êµ¬ì¡° (n-gram ì‚¬ìš©) | ì¼ë°˜í™” í•™ìŠµ | 1.3 |
| **Negative** | ì™„ì „íˆ ë‹¤ë¥¸ ì½”ë“œ | ê¸°ë³¸ ìŒì„± ìƒ˜í”Œ | 1.0 |

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Contrastive Learning Model     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Text Encoder â”‚  â”‚  AST-GNN    â”‚  â”‚
â”‚  â”‚ (Transformer)â”‚  â”‚ (2-layer)   â”‚  â”‚
â”‚  â”‚  4LÃ—H384Ã—6H  â”‚  â”‚ GraphSAGE   â”‚  â”‚
â”‚  â”‚   FF=1024    â”‚  â”‚ d:128â†’256   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                  â†“                  â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚          â”‚ Fusion Layer â”‚           â”‚
â”‚          â”‚ Concat(640)  â”‚           â”‚
â”‚          â”‚   Linearâ†’512 â”‚           â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                 â†“                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚          â”‚ Projection + â”‚           â”‚
â”‚          â”‚ L2 Normalize â”‚           â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ í•˜ì´í¼íŒŒë¼ë¯¸í„°

- **Text Encoder**: 4 layers, 6 heads, hidden 384, FFN 1024
- **AST-GNN**: 2 layers GraphSAGE, 128â†’256
- **Loss**: CE(0.3) + BCE(1.0), Ï„=0.2
- **Optimizer**: AdamW, lr=2e-4, warmup=1000

### ğŸ’­ íšŒê³ 

**ì¥ì **
- AST êµ¬ì¡° ì •ë³´ ëª…ì‹œì  ë°˜ì˜
- Hard negativeë¡œ ë¯¸ë¬˜í•œ ì°¨ì´ í•™ìŠµ

**ê°œì„  í•„ìš”**
- CV ê¸°ë²•ì˜ ì½”ë“œ ë„ë©”ì¸ ì ì‘ ë¶€ì¡± (MLMê³¼ ê°™ì€ ì˜ë¯¸ í•™ìŠµ ë³´ê°• í•„ìš”ì„±)
- êµ¬í˜„ ë³µì¡ë„ ë†’ìŒ
- íƒœìŠ¤í¬ íŠ¹í™” ìˆ˜ì • í•„ìš”

---

## 2ï¸âƒ£ ì¡°ë³‘ë¥  - Custom BERT

### ğŸ¯ í•µì‹¬ ì»¨ì…‰

ì²˜ìŒë¶€í„° ë¹Œë“œí•œ **BERT ì•„í‚¤í…ì²˜**ë¡œ CodeNetì—ì„œ MLM í•™ìŠµ

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

| í•­ëª© | ê°’ |
| --- | --- |
| **ì–´íœ˜ í¬ê¸°** | 32,005 |
| **íˆë“  ì°¨ì›** | 512 |
| **ë ˆì´ì–´ ìˆ˜** | 12 |
| **ì–´í…ì…˜ í—¤ë“œ** | 8 |
| **FFN ì°¨ì›** | 2,048 |
| **ë“œë¡­ì•„ì›ƒ** | 0.1 |

```
Input IDs
    â†“
Token Embeddings (32005 Ã— 512)
    â†“
Position Embeddings (max_pos Ã— 512)
    â†“
LayerNorm + Dropout
    â†“
12Ã— Transformer Encoder Layer
    â”œâ”€ Multi-Head Attention (8 heads)
    â”œâ”€ FFN (512 â†’ 2048 â†’ 512)
    â””â”€ Residual + LayerNorm
    â†“
MLM Head
    â””â”€ Linear (512 â†’ 32005)
```

### ğŸ“š Pretraining: MLM

**ë§ˆìŠ¤í‚¹ ì „ëµ**
- **ë§ˆìŠ¤í‚¹ ë¹„ìœ¨**: 15%
- **ë§ˆìŠ¤í‚¹ ê·œì¹™**: 80% [MASK], 10% ì›ë³¸, 10% ëœë¤

**í•™ìŠµ ì„¤ì •**
- learning_rate: 5e-4
- batch_size: 32
- epochs: 10
- optimizer: adamw_torch_fused
- Dataset: CodeNet

### ğŸ¯ Fine-tuning

**êµ¬ì¡° ë³€ê²½**
- MLM Head ì œê±°
- Sequence Classification Head ì¶”ê°€ (Linear 512â†’2)

### ğŸ“Š ê²°ê³¼

- **Public**: 0.8632535708
- **Private**: 0.8631608236

### ğŸ’­ ì˜ì˜

- ê¸°ë³¸ì— ì¶©ì‹¤í•œ êµ¬í˜„
- Baseline ì—­í• 
- í™•ì¥ ê°€ëŠ¥ì„±

---

## 3ï¸âƒ£ í™©í˜¸ì„± - RoBERTa-small

### ğŸ¯ í•µì‹¬ ì»¨ì…‰

**RoBERTa ìµœì í™” ê¸°ë²•**ì„ ì ìš©í•œ íš¨ìœ¨ì  ì‚¬ì „í•™ìŠµ

### ğŸ”¤ Tokenizer: Unigram (32k)

**Special Tokens ì„¤ê³„**

```python
# êµ¬ì¡° ë§ˆì»¤<indent>, <dedent>, <newline># ë¦¬í„°ëŸ´ ì¶”ìƒí™”<str>, <num># ê¸°ë³¸ í† í°<pad>, <s>, </s>, <mask>, <nl>, <py>, <unk>
```

**ì„ íƒ ì´ìœ **
- âœ… í¬ê·€ ì‹ë³„ì ë¶„ì ˆ ì•ˆì •ì„± â†‘
- âœ… 400MB ì†Œê·œëª¨ ë°ì´í„° ìˆ˜ë ´ ë¹ ë¦„
- âœ… OOV ì–µì œ íš¨ê³¼
- âœ… ê°œë… ì¼ë°˜í™” (`<str>`, `<num>`)
- âœ… êµ¬ì¡° ì´í•´ (`<indent>`, `<dedent>`)

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°: RoBERTa-small

```
Input (Token IDs)
    â†“
Embeddings
    â”œâ”€ Token Embedding (32000 Ã— 512)
    â”œâ”€ Position Embedding (514 Ã— 512)
    â””â”€ LayerNorm + Dropout
    â†“
6Ã— Transformer Encoder Layer
    â”œâ”€ Multi-Head Self-Attention (8 heads)
    â””â”€ Position-wise FFN (512â†’2048â†’512)
    â†“
MLM Head
    â””â”€ Decoder (512 â†’ 32000)
```

**ì„¸ë¶€ êµ¬ì„±**
- Embeddings: word(32000Ã—512) + position(514Ã—512)
- Encoder Layers: 6 layers
- Attention: 8 heads, d_model=512
- FFN: 512 â†’ 2048 â†’ 512 + GELU
- Total Params: ~50-60M

### ğŸ“š Pretraining: MLM

**í•™ìŠµ ì² í•™**
> â€œì‚¬ëŒì´ ë¬¸ë§¥ì„ ì´í•´í•´ì•¼ ë¹ˆì¹¸ì„ ì±„ìš°ë“¯, ëª¨ë¸ë„ ë¬¸ë§¥ì  í‘œí˜„ì„ í•™ìŠµí•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒâ€

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**
- epochs: 2
- batch_size: 40
- learning_rate: 6e-4
- weight_decay: 0.01
- warmup_ratio: 0.06
- mlm_probability: 0.15

### ğŸ“ˆ í•™ìŠµ ê²°ê³¼

```
Epoch 1:
  - eval_perplexity: 1.7 â†’ 1.3
  - eval_loss: 0.55 â†’ 0.23

Epoch 2:
  - eval_perplexity: 1.3 â†’ 1.25
  - eval_loss: 0.23 â†’ 0.22
```

### ğŸ† ì„±ëŠ¥ ë¹„êµ

### **1) Pretrained Model ë¹„êµ**

| ëª¨ë¸ | Tokenizer Vocab | Public | Private |
| --- | --- | --- | --- |
| **HuggingFace RoBERTa** | 52,000 | **94.90** | **94.75** |
| **Custom RoBERTa** | 32,000 | 91.67 | 91.64 |

### **2) ë°ì´í„° ì „ì²˜ë¦¬ & í¬ê¸° ì˜í–¥**

| ì„¤ì • | ì „ì²˜ë¦¬ | ë°ì´í„° í¬ê¸° | Public | Private |
| --- | --- | --- | --- | --- |
| **preA_60000** | ë°©ë²• A | 60,000 | **91.67** | **91.64** |
| **preB_60000** | ë°©ë²• B | 60,000 | 87.13 | 87.09 |
| **preB_180000** | ë°©ë²• B | 180,000 | 88.85 | 88.94 |

### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

**ì „ì²˜ë¦¬ ë°©ë²• Aì˜ ìš°ìˆ˜ì„±**

```python
# ì¢Œì¸¡ ì ˆë‹¨ (ì½”ë“œ ëë¶€ë¶„ ìš°ì„ )tokenizer.truncation_side = "left"# ê°„ë‹¨í•˜ë©´ì„œ íš¨ê³¼ì ì¸ ì •ì œdef simple_clean(code):
    return code.strip()
```

**ë°ì´í„° í¬ê¸°ì˜ ì¤‘ìš”ì„±**
- ì „ì²˜ë¦¬ ê°œì„ : +4.5%p
- ë°ì´í„° 3ë°°: +1.7%p

---

## 4ï¸âƒ£ ì˜¤ì •íƒ - Role Embedding CodeBERT

### ğŸ¯ í•µì‹¬ ì»¨ì…‰

**êµ¬ë¬¸ ì •ë³´(Role)ë¥¼ ì„ë² ë”©ì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€**í•œ CodeBERT

### ğŸ§© ì½”ë“œ ì •ê·œí™” íŒŒì´í”„ë¼ì¸

**ëª©í‘œ**: ì½”ë“œì˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ í‘œë©´ì  ìš”ì†Œë¥¼ í‘œì¤€í™”

**ì „ì²´ íë¦„**

```
ì£¼ì„ ì œê±° â†’ ì‹ë³„ì ìµëª…í™” â†’ ê°œí–‰ ì •ë¦¬ â†’ Black í¬ë§· â†’ SHA1 í•´ì‹œ â†’ ì¤‘ë³µ ì œê±°
```

**â‘  ì£¼ì„ ì œê±°**
- tokenize ëª¨ë“ˆ ê¸°ë°˜ COMMENT í† í° í•„í„°ë§

**â‘¡ ìµëª…í™” (Alpha Rename)**
- AST ê¸°ë°˜ ìŠ¤ì½”í”„ ë‹¨ìœ„ ì¼ê´€ ì¹˜í™˜
- í•¨ìˆ˜: func1, func2, â€¦
- í´ë˜ìŠ¤: Cls1, Cls2, â€¦
- ë³€ìˆ˜: v1, v2, â€¦ (ìŠ¤ì½”í”„ë³„)

**â‘¢ ê°œí–‰ ì •ë¦¬**
- ì—°ì†ëœ ë¹ˆ ì¤„ì„ í•˜ë‚˜ë¡œ ì••ì¶•

**â‘£ Black í¬ë§·íŒ…**
- ë“¤ì—¬ì“°ê¸°, ê´„í˜¸, ê³µë°± í†µì¼
- line_length=88

**â‘¤ í•´ì‹œ ìƒì„± (SHA-1)**
- ì •ê·œí™”ëœ ì½”ë“œë¥¼ ê³ ìœ  í•´ì‹œë¡œ ë³€í™˜
- ë™ì¼ ì½”ë“œ êµ¬ì¡° ë¹ ë¥´ê²Œ ë¹„êµ

**â‘¥ ì¤‘ë³µ ì œê±°**
- text_norm_sha1 ê¸°ì¤€ìœ¼ë¡œ drop_duplicates
- 604,124ê°œ ì¤‘ë³µí–‰ ì œê±° (2,639,300ê°œ ë‚¨ìŒ)

### ğŸ”¤ Tokenizer: BPE (50,265)

- ByteLevelBPE ì‚¬ìš©
- RoBERTaì™€ ë™ì¼í•œ vocab_size
- Special tokens: [CLS], [PAD], [SEP], [UNK], [MASK]

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°: CodeBERT + Role Embedding

```
Input
    â†“
Embeddings = Token + Position + Segment + Role
    â†“
LayerNorm + Dropout
    â†“
12Ã— Transformer Encoder (norm-first, GELU)
    â”œâ”€ Multi-Head Attention
    â””â”€ FFN (768 â†’ 3072 â†’ 768)
    â†“
MLM Head + Role Head
```

**Role Embedding ì¶”ê°€**
- vocab_size: 32,005
- role_vocab_size: 7
- ROLE2ID: {PAD, UNK, KEYWORD, IDENTIFIER, OP, LITERAL, SEP}

**íœ´ë¦¬ìŠ¤í‹± ì—­í•  íƒœê¹…**
- íŒŒì´ì¬ í‚¤ì›Œë“œ/ì—°ì‚°ì/ìˆ«ì/ë¬¸ìì—´ ì •ê·œì‹ ê¸°ë°˜

### ğŸ“š Pretraining Task: MLM + Role

**ì†ì‹¤ í•¨ìˆ˜**

```python
mlm_loss = CrossEntropy(masked tokens)
role_loss = CrossEntropy(role labels)
total_loss = Î»_mlm * mlm_loss + Î»_role * role_loss
```

**í•™ìŠµ ì„¤ì •**
- learning_rate: 1e-4
- batch_size: 32 (GPU ì œì•½)
- epochs: 3
- warmup_steps: 10,000
- optimizer: AdamW
- scheduler: OneCycleLR

### ğŸ¯ Fine-tuning

**êµ¬ì¡°**

```
RoleBERT (pretrained)
    â†“
Remove: MLM Head, Role Head
    â†“
Add: Classification Head
    â”œâ”€ Dense (768 â†’ 768)
    â”œâ”€ Dropout
    â””â”€ Output (768 â†’ 2)
```

**í•™ìŠµ ì„¤ì •**
- batch_size: 16
- learning_rate: 2e-5
- epochs: 3
- warmup_steps: 100

**ê²°ê³¼**

```
Validation ACC: 0.9458
Validation F1: 0.9470
AUC: 0.9881
Best threshold: 0.50
```

### ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´

**Role Embeddingì˜ ì¥ì **
- êµ¬ë¬¸ ì •ë³´ë¥¼ í† í° ì„ë² ë”©ì— ì§ì ‘ ì£¼ì…
- KEYWORD/IDENTIFIER/OP/LITERAL ë“± êµ¬ë¶„
- ì½”ë“œ êµ¬ì¡° ì´í•´ì— ìœ ë¦¬

**ì„¤ê³„ ì˜ë„**
- ì™„ì „ ìë¦½í˜•: í† í¬ë‚˜ì´ì €/ëª¨ë¸ ëª¨ë‘ ë‚´ ì½”í¼ìŠ¤ì— íŠ¹í™”
- ë©€í‹° ì˜¤ë¸Œì í‹°ë¸Œ: MLM(ë¬¸ë§¥) + Role(êµ¬ë¬¸) ë™ì‹œ í•™ìŠµ
- ê°„ê²°í•œ ì—”ì§„: PyTorch ê¸°ë³¸ TransformerEncoder ì‚¬ìš©

---

## 5ï¸âƒ£ ì´í™ê¸° - CodeBERT (MLM + RTD)

### ğŸ¯ í•µì‹¬ ì»¨ì…‰

**HuggingFaceì˜ ì‚¬ì „í•™ìŠµëœ CodeBERT**ë¥¼ ì‚¬ìš©í•˜ì—¬ Fine-tuning

### ğŸ“¦ ì „ì²´ í”Œë¡œìš°

```
[Phase 1: ë°ì´í„° ì¤€ë¹„]
â†’ 260ë§Œê°œ Python ì½”ë“œ ìˆ˜ì§‘
â†’ BPE í† í¬ë‚˜ì´ì € í•™ìŠµ (vocab 50,265ê°œ)
â†’ ì „ì²´ ë°ì´í„° í† í°í™” (512 í† í°)

[Phase 2: ì‚¬ì „í•™ìŠµ]
â†’ CodeBERT ëª¨ë¸ from scratch êµ¬ì¶•
   - 12-layer Transformer Encoder
   - Hidden size: 768
   - Attention heads: 12
â†’ MLM + RTD ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
   - MLM: 15% ë§ˆìŠ¤í‚¹í•˜ì—¬ ì˜ˆì¸¡
   - RTD: êµì²´ëœ í† í° ê°ì§€
â†’ 3 epochs í•™ìŠµ

[Phase 3: íŒŒì¸íŠœë‹]
â†’ ì½”ë“œ ìœ ì‚¬ë„ ë¶„ë¥˜ íƒœìŠ¤í¬
â†’ 17,970 pairs (similar 0/1)
â†’ Cross-Encoder ë°©ì‹
```

### ğŸ”¤ Tokenizer: BPE (50,265)

**í•™ìŠµ ë°©ì‹**
- ByteLevelBPETokenizer ì‚¬ìš©
- CodeNet ì½”í¼ìŠ¤ë¡œ ì§ì ‘ í•™ìŠµ
- min_frequency: 2
- Special tokens: [CLS], [PAD], [SEP], [UNK], [MASK]

### ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

```
Input â†’ Embeddings â†’ 12 Transformer Layers â†’ MLM/RTD Heads
```

**Embeddings**

```
Token Embeddings (50265 Ã— 768)
    +
Position Embeddings (512 Ã— 768)
    +
Token Type Embeddings (2 Ã— 768)
    â†“
LayerNorm + Dropout
```

**Transformer Layer**

```
Multi-Head Attention (12 heads, hidden 768)
    â†“
Residual + LayerNorm
    â†“
Feed Forward (768 â†’ 3072 â†’ 768)
    â†“
Residual + LayerNorm
```

### ğŸ“š Pretraining: MLM + RTD

**1) MLM (Masked Language Modeling)**
- ì…ë ¥ ì‹œí€€ìŠ¤ì˜ 15% í† í°ì„ ë§ˆìŠ¤í‚¹
- ëª¨ë¸ì´ ì›ë˜ í† í°ì„ ì˜ˆì¸¡

**2) RTD (Replaced Token Detection)**
- ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì¼ë¶€ í† í°ì„ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ì¹˜í™˜
- ëª¨ë¸ì´ ì›ë³¸(0) vs ì¹˜í™˜(1) íŒë³„

**ì†ì‹¤ í•¨ìˆ˜**

```python
mlm_loss = CrossEntropy(masked positions)
rtd_loss = CrossEntropy(all positions)
total_loss = mlm_loss + rtd_loss
```

**í•™ìŠµ ì„¤ì •**
- batch_size: 16
- epochs: 3
- learning_rate: 1e-4
- warmup_steps: 10,000
- optimizer: AdamW
- scheduler: OneCycleLR

### ğŸ¯ Fine-tuning

**ëª¨ë¸ ë¡œë“œ**

```python
MODEL_NAME = "microsoft/codebert-base"encoder = RobertaModel.from_pretrained(MODEL_NAME)
```

**ì „ì²´ êµ¬ì¡°**

```
ì´ íŒŒë¼ë¯¸í„°: 124,055,810ê°œ

[Encoder: 124,054,272ê°œ] â† ì‚¬ì „í•™ìŠµëœ ë¶€ë¶„
â”œâ”€ Embeddings: 38,603,520ê°œ
â””â”€ 12Ã— Transformer Layers: 85,450,752ê°œ

[Classifier: 1,538ê°œ] â† ìƒˆë¡œ ì¶”ê°€
â””â”€ Linear: 768 Ã— 2 + 2
```

**Classifier êµ¬ì¡°**

```
CodeBERT Encoder
    â†“
[CLS] hidden vector (768)
    â†“
Dropout
    â†“
Linear (768 â†’ 2)
    â†“
Logits â†’ Softmax â†’ Probability
```

**í•™ìŠµ ê¸°ë²•**

**â‘  AdamW (Adam with Weight Decay)**
- ì ì‘ì  í•™ìŠµë¥ : ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ë‹¤ë¥¸ í•™ìŠµë¥ 
- 1ì°¨ ëª¨ë©˜íŠ¸ (m): Gradient ì´ë™ í‰ê· 
- 2ì°¨ ëª¨ë©˜íŠ¸ (v): Gradient ì œê³± ì´ë™ í‰ê· 
- Weight Decayë¥¼ gradientê°€ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ì— ì§ì ‘ ì ìš©

**â‘¡ Linear Warmup + Decay Scheduler**

```
Learning Rate
 â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
 â”‚ â•±                              â•²
 â”‚â•±                                â•²___
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Steps
  â†‘         â†‘                      â†‘
Start   Warmup End              End
```

- Warmup: í•™ìŠµë¥ ì„ 0ì—ì„œ ëª©í‘œ ê°’ê¹Œì§€ ì„œì„œíˆ ì¦ê°€
- Decay: Warmup ì´í›„ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œ

**í•™ìŠµ ì„¤ì •**
- batch_size: 64
- max_length: 512
- learning_rate: 2e-5
- epochs: 3
- warmup_steps: 100

### ğŸ“Š ê²°ê³¼

**ìµœì¢… ì„±ëŠ¥**
- **Public**: 0.9284177333
- **Private**: 0.9285793783

### ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

**HuggingFace ëª¨ë¸ ì‚¬ìš©ì˜ ì¥ì **
- ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ í™œìš©
- ì•ˆì •ì ì¸ ì„±ëŠ¥ ë³´ì¥
- êµ¬í˜„ ê°„ê²°

**Transfer Learningì˜ ì›ë¦¬**
- ì‚¬ì „í•™ìŠµ: ëŒ€ëŸ‰ì˜ ì½”ë“œ ë°ì´í„°ë¡œ ì¼ë°˜ì ì¸ ì½”ë“œ ì´í•´ ëŠ¥ë ¥ ìŠµë“
- Fine-tuning: ìš°ë¦¬ taskì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •
- EncoderëŠ” ì²œì²œíˆ, ClassifierëŠ” ë¹ ë¥´ê²Œ í•™ìŠµ

**Why Full Fine-tuning?**
- ì½”ë“œ ìœ ì‚¬ë„ íŒë‹¨ì€ ì‚¬ì „í•™ìŠµ ë•Œ ë°°ìš°ì§€ ì•Šì€ ìƒˆë¡œìš´ task
- ì½”ë“œ í‘œí˜„ ìì²´ë¥¼ taskì— ë§ê²Œ ì¡°ì •í•´ì•¼ í•¨
- LoRA/PEFTëŠ” ìƒˆë¡œìš´ ê´€ê³„ í•™ìŠµì— ì œí•œì 

---

## ğŸ“Š ì¢…í•© ë¹„êµ

### ì•„í‚¤í…ì²˜ ë¹„êµ

| êµ¬ë¶„ | ì´ì„œìœ¨ | ì¡°ë³‘ë¥  | í™©í˜¸ì„± | ì˜¤ì •íƒ | ì´í™ê¸° |
| --- | --- | --- | --- | --- | --- |
| **êµ¬ì¡°** | Dual Encoder | Single BERT | RoBERTa-small | CodeBERT + Role | CodeBERT |
| **íŠ¹ìˆ˜ ëª¨ë“ˆ** | AST-GNN | ì—†ìŒ | ì—†ìŒ | Role Embedding | RTD Head |
| **íŒŒë¼ë¯¸í„°** | ~50M | ~60M | ~50-60M | ~60M | ~124M |
| **í•™ìŠµ ëª©í‘œ** | Contrastive | MLM | MLM | MLM + Role | MLM + RTD |

### Tokenizer ë¹„êµ

| í•­ëª© | ì´ì„œìœ¨ | ì¡°ë³‘ë¥  | í™©í˜¸ì„± | ì˜¤ì •íƒ | ì´í™ê¸° |
| --- | --- | --- | --- | --- | --- |
| **ë°©ì‹** | Unigram | SentencePiece | Unigram | BPE | BPE |
| **Vocab Size** | 32,000 | 32,005 | 32,000 | 50,265 | 50,265 |
| **í•™ìŠµ ë°ì´í„°** | CodeNet (400MB) | CodeNet | CodeNet (400MB) | CodeNet | CodeNet |
| **íŠ¹ì§•** | í¬ê·€ì–´ ì•ˆì • | ê¸°ë³¸ | í¬ê·€ì–´ ì•ˆì • | RoBERTa í‘œì¤€ | RoBERTa í‘œì¤€ |

### í•™ìŠµ ê¸°ë²• ë¹„êµ

| í•­ëª© | ì´ì„œìœ¨ | ì¡°ë³‘ë¥  | í™©í˜¸ì„± | ì˜¤ì •íƒ | ì´í™ê¸° |
| --- | --- | --- | --- | --- | --- |
| **Optimizer** | AdamW | AdamW | AdamW | AdamW | AdamW |
| **Learning Rate** | 2e-4 | 5e-4 | 6e-4 | 1e-4 | 2e-5 |
| **Scheduler** | Cosine | Cosine | Cosine | OneCycle | Linear + Warmup |
| **Warmup** | 1000 steps | - | 6% | 10k steps | 100 steps |
| **Weight Decay** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |

### ì¥ë‹¨ì  ë¹„êµ

| ì ‘ê·¼ë²• | ì¥ì  | ë‹¨ì  |
| --- | --- | --- |
| **ì´ì„œìœ¨ - ëŒ€ì¡°í•™ìŠµ** | â€¢ AST êµ¬ì¡° ì •ë³´ ëª…ì‹œì  í™œìš©â€¢ Hard negativeë¡œ ì„¸ë°€í•œ í•™ìŠµâ€¢ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì§ì ‘ í•™ìŠµ | â€¢ êµ¬í˜„ ë³µì¡ë„ ë†’ìŒâ€¢ CVâ†’Code ì ì‘ ë¶€ì¡±â€¢ í•™ìŠµ ë¶ˆì•ˆì • ê°€ëŠ¥ì„± |
| **ì¡°ë³‘ë¥  - Custom BERT** | â€¢ ê¸°ë³¸ì— ì¶©ì‹¤í•œ êµ¬í˜„â€¢ Baseline ì—­í• â€¢ í™•ì¥ ê°€ëŠ¥ì„± | â€¢ ë‹¨ìˆœ êµ¬ì¡°â€¢ íŠ¹í™” ìµœì í™” ë¶€ì¡±â€¢ ì‚¬ì „í•™ìŠµ ë°ì´í„° ì œì•½ |
| **í™©í˜¸ì„± - RoBERTa** | â€¢ ì•ˆì •ì  ìˆ˜ë ´â€¢ íš¨ìœ¨ì  í•™ìŠµâ€¢ ìµœê³  ì„±ëŠ¥ (Custom ì¤‘)â€¢ êµ¬í˜„ ê°„ê²° | â€¢ êµ¬ì¡° ì •ë³´ ë¯¸í™œìš©â€¢ í‘œì¤€ ì ‘ê·¼ë²•â€¢ HF ëª¨ë¸ ëŒ€ë¹„ ë‚®ìŒ |
| **ì˜¤ì •íƒ - Role Embedding** | â€¢ êµ¬ë¬¸ ì •ë³´ ëª…ì‹œì  í™œìš©â€¢ ì™„ì „ ìë¦½í˜•â€¢ ì½”ë“œ ì •ê·œí™” ì²´ê³„ì  | â€¢ êµ¬í˜„ ë³µì¡ë„ ì¤‘ê°„â€¢ Role íƒœê¹… íœ´ë¦¬ìŠ¤í‹± ì˜ì¡´â€¢ ì„±ëŠ¥ ê²€ì¦ í•„ìš” |
| **ì´í™ê¸° - MLM+RTD** | â€¢ HF ëª¨ë¸ í™œìš©â€¢ ìµœê³  ì„±ëŠ¥â€¢ ì•ˆì •ì  í•™ìŠµâ€¢ êµ¬í˜„ ê°„ê²° | â€¢ ëŒ€ê·œëª¨ ëª¨ë¸ í•„ìš”â€¢ êµ¬ì¡° ì •ë³´ ë¯¸í™œìš©â€¢ ì‚¬ì „í•™ìŠµ ì˜ì¡´ë„ ë†’ìŒ |

---

## ğŸ’¡ í–¥í›„ ê°œì„  ë°©í–¥

### ê³µí†µ ê°œì„  ë°©í–¥

**1. ë°ì´í„° ì¦ê°•**
- ë” í° ë°ì´í„°ì…‹ í™œìš©
- ë‹¤ì–‘í•œ ì–¸ì–´ í¬í•¨
- Back-translation ë“± ì¦ê°• ê¸°ë²•

**2. ì•™ìƒë¸”**
- ë‹¤ì–‘í•œ ì ‘ê·¼ë²• ê²°í•©
- ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í™œìš©
- Soft voting / Hard voting

**3. êµ¬ì¡° ì •ë³´ í†µí•©**
- AST ì •ë³´ ê²½ëŸ‰ í†µí•©
- Control flow ê³ ë ¤
- Data flow ë¶„ì„

---

## ğŸ“Œ ê²°ë¡ 

**ë¬´ì—‡ì´ ì„±ëŠ¥ì„ ê²°ì •í•˜ëŠ”ê°€?**

1. **ì‚¬ì „í•™ìŠµ ë°ì´í„°** (ê°€ì¥ ì¤‘ìš”)
    - ëŒ€ê·œëª¨ > ì†Œê·œëª¨
    - HF ëª¨ë¸ > Custom ëª¨ë¸
2. **ì „ì²˜ë¦¬ í’ˆì§ˆ**
    - í’ˆì§ˆ > ì–‘
    - ì¢Œì¸¡ ì ˆë‹¨ íš¨ê³¼ì 
3. **í•™ìŠµ ì „ëµ**
    - AdamW + Warmup Scheduler
    - Full Fine-tuning
    - ì•ˆì •ì  í•™ìŠµ
4. **ëª¨ë¸ êµ¬ì¡°**
    - ë‹¨ìˆœí•˜ê³  ìµœì í™”ëœ ì ‘ê·¼
    - êµ¬ì¡° ì •ë³´ëŠ” ë³´ì¡°ì 
5. **í˜ì‹ ê³¼ ì‹¤í—˜**
    - ë‹¤ì–‘í•œ ì‹œë„ì˜ ê°€ì¹˜
    - ì‹¤íŒ¨ì—ì„œ ë°°ìš°ëŠ” êµí›ˆ

---