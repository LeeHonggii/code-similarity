# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c0wHoE8XSHrA5rmL6fukKHH7Iun57tAg
"""

from google.colab import drive
drive.mount('/content/drive')

!pip -q install pyarrow fastparquet duckdb polars

import pyarrow.parquet as pq

par_path = "/content/drive/MyDrive/code_corpus_processed.parquet"
pf = pq.ParquetFile(par_path)

# Ïä§ÌÇ§Îßà
print("Schema:\n", pf.schema)

# ÌñâÍ∑∏Î£π Í∞úÏàò/Ìñâ Ïàò/Ïó¥ Ïàò
print(f"\nNum row groups: {pf.num_row_groups}")
print(f"Num columns: {len(pf.schema)}")
try:
    # Parquet metadataÏóê Ï¥ù row ÏàòÍ∞Ä Îì§Ïñ¥ÏûàÏúºÎ©¥ ÌëúÏãú
    print("Total rows (from metadata):", pf.metadata.num_rows)
except:
    pass

# Í∞Å ÌñâÍ∑∏Î£πÎãπ ÎåÄÎûµ ÌÅ¨Í∏∞/ÌñâÏàò
for i in range(pf.num_row_groups):
    rg = pf.metadata.row_group(i)
    print(f"RowGroup {i}: rows={rg.num_rows}, total_byte_size={rg.total_byte_size}")

# ============================================================
# Pair Build
#  - Positive:        same problem AC‚ÄìAC
#  - Hard Negative:   same problem AC‚ÄìWA/RE/TLE
#  - Easy Negative:   different problem AC‚ÄìAC (random bucket)
#  - Semi-Hard Neg.:  different problem AC‚ÄìAC (TF-IDF nearest in bucket)
#  - Final union with unified 8-column schema
# ============================================================

!pip -q install duckdb pyarrow pandas scikit-learn

import os, textwrap, duckdb, pandas as pd, numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors

# ----------------------------
# ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï
# ----------------------------
PAR_PATH = "/content/drive/MyDrive/code_corpus_processed.parquet"   # text_norm Ìè¨Ìï®Îêú Ï†ÑÏ≤òÎ¶¨ Parquet
OUT_DIR  = "/content/drive/MyDrive/pairs_out_v4"
os.makedirs(OUT_DIR, exist_ok=True)

LANG_FILTER   = None            # "Python" or None
MIN_CODE_LEN  = 0
HARD_STATUSES = ["WA","RE","TLE"]

# ÌÉÄÍπÉ Ïåç Ïàò
TARGET_TOTAL_PAIRS = 200_000
TARGET_PER_CLASS   = TARGET_TOTAL_PAIRS // 4  # pos/hard/easy/semi_hard Í∑†Ìòï

# Ïä§ÏºÄÏùº ÌååÎùºÎØ∏ÌÑ∞
PER_PROBLEM_POS_CAP  = 50
PER_PROBLEM_HARD_CAP = 50
EASY_BUCKETS         = 1024
PER_BUCKET_EASY_CAP  = 80
SAMPLE_MULTIPLIER    = 4

# Semi-hard ÎßàÏù¥Îãù
SEMI_BUCKETS              = 1024
SEMI_PER_BUCKET           = 40
SEMI_K_NEIGHBORS          = 6
SEMI_MIN_SIM              = 0.15
SEMI_MAX_TEXTS_PER_BUCKET = 1200  # Î©îÎ™®Î¶¨ ÏïàÏ†ÑÏû•Ïπò

# ----------------------------
# DuckDB ÏÑ∏ÏÖò
# ----------------------------
con = duckdb.connect()
con.execute("PRAGMA threads=8;")
con.execute("PRAGMA temp_directory='/content';")

# 0) Î°úÎìú
con.execute(f"CREATE OR REPLACE VIEW subs AS SELECT * FROM read_parquet('{PAR_PATH}');")

# 1) Ï†ïÍ∑úÌôî
con.execute(textwrap.dedent("""
CREATE OR REPLACE VIEW subs_norm AS
SELECT
  *,
  CASE
    WHEN lower(status) LIKE '%accept%'                         THEN 'AC'
    WHEN lower(status) LIKE '%wrong answer%'                   THEN 'WA'
    WHEN lower(status) LIKE '%presentation error%'             THEN 'WA'
    WHEN lower(status) LIKE '%time limit exceeded%' OR lower(status) LIKE 'tle%' THEN 'TLE'
    WHEN lower(status) LIKE '%runtime error%' OR lower(status) LIKE 'rte%'       THEN 'RE'
    WHEN lower(status) LIKE '%compile error%'  OR lower(status) LIKE '%syntax%'   THEN 'CE'
    WHEN lower(status) LIKE '%memory limit exceeded%'          THEN 'MLE'
    WHEN lower(status) LIKE '%output limit exceeded%'          THEN 'OLE'
    ELSE upper(status)
  END AS status_norm,
  CASE
    WHEN lower(language) LIKE '%python%' OR language LIKE 'Py%' THEN 'Python'
    ELSE language
  END AS language_norm
FROM subs;
"""))

# 2) ÌÅ¥Î¶∞ ÌïÑÌÑ∞
conds = ["text_norm IS NOT NULL", "text_norm_sha1 IS NOT NULL", f"length(text_norm) >= {MIN_CODE_LEN}"]
if LANG_FILTER:
    conds.append(f"language_norm = '{LANG_FILTER}'")
WHERE_CLEAN = " AND ".join(conds)

con.execute(f"CREATE OR REPLACE VIEW clean AS SELECT * FROM subs_norm WHERE {WHERE_CLEAN};")

# 3) AC/WA Î∂ÑÍ∏∞ + Ï§ëÎ≥µ Ï†úÍ±∞
con.execute("CREATE OR REPLACE VIEW ac_raw AS SELECT * FROM clean WHERE status_norm='AC';")
hs = ",".join([f"'{s}'" for s in HARD_STATUSES])
con.execute(f"CREATE OR REPLACE VIEW wa_raw AS SELECT * FROM clean WHERE status_norm IN ({hs});")

con.execute("""
CREATE OR REPLACE VIEW ac AS
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY problem_id, text_norm_sha1 ORDER BY date) rn
  FROM ac_raw
) t WHERE rn=1;
""")
con.execute("""
CREATE OR REPLACE VIEW wa AS
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY problem_id, text_norm_sha1 ORDER BY date) rn
  FROM wa_raw
) t WHERE rn=1;
""")

# 4-A) Positive: same problem AC‚ÄìAC
con.execute(f"""
CREATE OR REPLACE TEMP TABLE pos_ids AS
WITH ac_pick AS (
  SELECT submission_id, problem_id, text_norm_sha1,
         ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) AS rn
  FROM ac
)
SELECT a.submission_id AS sub_a,
       b.submission_id AS sub_b,
       a.problem_id    AS problem_id
FROM ac_pick a
JOIN ac_pick b
  ON a.problem_id=b.problem_id AND a.rn%2=1 AND b.rn=a.rn+1
WHERE a.text_norm_sha1<>b.text_norm_sha1
QUALIFY ROW_NUMBER() OVER (PARTITION BY a.problem_id ORDER BY random()) <= {PER_PROBLEM_POS_CAP};
""")

# 4-B) Hard: same problem AC‚ÄìWA
con.execute(f"""
CREATE OR REPLACE TEMP TABLE hard_ids AS
WITH ac_pick AS (
  SELECT submission_id, problem_id, text_norm_sha1,
         ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) AS rn,
         COUNT(*) OVER (PARTITION BY problem_id) AS ac_n
  FROM ac
),
wa_pick AS (
  SELECT submission_id, problem_id, text_norm_sha1,
         ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) AS rn,
         COUNT(*) OVER (PARTITION BY problem_id) AS wa_n
  FROM wa
),
paired AS (
  SELECT a.submission_id AS sub_a, w.submission_id AS sub_b,
         a.problem_id, a.rn, a.ac_n, w.wa_n
  FROM ac_pick a
  JOIN wa_pick w ON a.problem_id=w.problem_id AND a.rn=w.rn
)
SELECT sub_a, sub_b, problem_id
FROM paired
QUALIFY rn <= LEAST(ac_n, wa_n)
AND ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) <= {PER_PROBLEM_HARD_CAP};
""")

# 4-C) Easy: diff problem AC‚ÄìAC (ÎûúÎç§ Î≤ÑÌÇ∑)
con.execute(f"""
CREATE OR REPLACE TEMP VIEW ac_bucket AS
SELECT *,
       (abs(hash(text_norm_sha1)) % {EASY_BUCKETS}) AS bkt
FROM ac;
""")
con.execute(f"""
CREATE OR REPLACE TEMP TABLE easy_ids AS
WITH sampled AS (
  SELECT submission_id, problem_id, text_norm_sha1, bkt
  FROM ac_bucket
  QUALIFY ROW_NUMBER() OVER (PARTITION BY bkt ORDER BY random()) <= {PER_BUCKET_EASY_CAP} * {SAMPLE_MULTIPLIER}
),
s1 AS (
  SELECT submission_id, problem_id, text_norm_sha1, bkt,
         ROW_NUMBER() OVER (PARTITION BY bkt ORDER BY random()) AS rn
  FROM sampled
),
s2 AS (
  SELECT submission_id, problem_id, text_norm_sha1, bkt,
         ROW_NUMBER() OVER (PARTITION BY bkt ORDER BY random()) AS rn
  FROM sampled
)
SELECT a.submission_id AS sub_a,
       b.submission_id AS sub_b,
       a.bkt AS bkt
FROM s1 a
JOIN s2 b
  ON a.bkt=b.bkt AND a.rn=b.rn
WHERE a.problem_id<>b.problem_id
  AND a.text_norm_sha1<>b.text_norm_sha1
QUALIFY ROW_NUMBER() OVER (PARTITION BY a.bkt ORDER BY random()) <= {PER_BUCKET_EASY_CAP};
""")

# 4-D) Semi-Hard: diff problem AC‚ÄìAC (TF-IDF Í∑ºÏÇ¨ ÏµúÍ∑ºÏ†ë within bucket)
ac_bucket_df = con.execute(f"""
SELECT submission_id, problem_id, text_norm, (abs(hash(text_norm_sha1)) % {SEMI_BUCKETS}) AS bkt
FROM ac
""").df()

semi_pairs = []
rng = np.random.default_rng(0)
for bkt, grp in ac_bucket_df.groupby("bkt"):
    if len(grp) < 3:
        continue
    if len(grp) > SEMI_MAX_TEXTS_PER_BUCKET:
        grp = grp.sample(SEMI_MAX_TEXTS_PER_BUCKET, random_state=0)
    texts = grp["text_norm"].astype(str).tolist()
    sids  = grp["submission_id"].tolist()
    pids  = grp["problem_id"].tolist()

    vec = TfidfVectorizer(analyzer="char", ngram_range=(3,5), min_df=2)
    X = vec.fit_transform(texts)
    if X.shape[0] < 3:
        continue

    nn = NearestNeighbors(n_neighbors=min(SEMI_K_NEIGHBORS+1, len(texts)), metric="cosine", n_jobs=-1)
    nn.fit(X)
    dists, idxs = nn.kneighbors(X, return_distance=True)

    per_bucket = 0
    for i in range(len(texts)):
        if per_bucket >= SEMI_PER_BUCKET: break
        for jpos in range(1, idxs.shape[1]):
            j = idxs[i, jpos]
            if pids[i] == pids[j]:  # Í∞ôÏùÄ problem Ï†úÏô∏
                continue
            sim = 1.0 - dists[i, jpos]
            if sim < SEMI_MIN_SIM:
                continue
            a, b = (i, j) if sids[i] < sids[j] else (j, i)
            semi_pairs.append((sids[a], sids[b], bkt, sim))
            per_bucket += 1
            if per_bucket >= SEMI_PER_BUCKET: break

semi_df = pd.DataFrame(semi_pairs, columns=["sub_a","sub_b","bkt","sim"]).drop_duplicates(["sub_a","sub_b"])
print("Semi-hard pairs (final):", len(semi_df))

# 5) ÌÅ¥ÎûòÏä§Î≥Ñ ÌååÏùº ÏÉùÏÑ± (Ïó¨Í∏∞ÏÑú text_a/text_bÎäî text_norm = Ï†ïÍ∑úÌôî ÏΩîÎìú)
def copy_parquet(sql, path):
    con.execute(f"COPY ({sql}) TO '{path}' (FORMAT PARQUET, COMPRESSION ZSTD);")

# Positive (AC‚ÄìAC, same problem)
copy_parquet(f"""
  SELECT
    'positive' AS pair_type, 1 AS label,
    pa.sub_a, pa.sub_b,
    a.text_norm AS text_a, b.text_norm AS text_b,   -- ‚úÖ Ï†ïÍ∑úÌôî ÏΩîÎìú ÏÇ¨Ïö©
    a.problem_id AS problem_id_a, b.problem_id AS problem_id_b
  FROM (SELECT sub_a, sub_b FROM pos_ids) pa
  JOIN ac a ON pa.sub_a=a.submission_id
  JOIN ac b ON pa.sub_b=b.submission_id
""", f"{OUT_DIR}/pairs_pos.parquet")

# Hard (AC‚ÄìWA, same problem)
copy_parquet(f"""
  SELECT
    'hard_negative' AS pair_type, 0 AS label,
    ha.sub_a, ha.sub_b,
    a.text_norm AS text_a, w.text_norm AS text_b,   -- ‚úÖ Ï†ïÍ∑úÌôî ÏΩîÎìú ÏÇ¨Ïö©
    a.problem_id AS problem_id_a, w.problem_id AS problem_id_b
  FROM (SELECT sub_a, sub_b FROM hard_ids) ha
  JOIN ac a ON ha.sub_a=a.submission_id
  JOIN wa w ON ha.sub_b=w.submission_id
""", f"{OUT_DIR}/pairs_hard.parquet")

# Easy (AC‚ÄìAC, diff problem)
copy_parquet(f"""
  SELECT
    'easy_negative' AS pair_type, 0 AS label,
    ea.sub_a, ea.sub_b,
    a.text_norm AS text_a, b.text_norm AS text_b,   -- ‚úÖ Ï†ïÍ∑úÌôî ÏΩîÎìú ÏÇ¨Ïö©
    a.problem_id AS problem_id_a, b.problem_id AS problem_id_b
  FROM (SELECT sub_a, sub_b FROM easy_ids) ea
  JOIN ac a ON ea.sub_a=a.submission_id
  JOIN ac b ON ea.sub_b=b.submission_id
""", f"{OUT_DIR}/pairs_easy.parquet")

# Semi-hard (pandas join)
ac_table = con.execute("SELECT submission_id, problem_id, text_norm FROM ac").df().set_index("submission_id")
semi_join = semi_df.join(ac_table, on="sub_a").rename(columns={"problem_id":"problem_id_a","text_norm":"text_a"})
semi_join = semi_join.join(ac_table, on="sub_b").rename(columns={"problem_id":"problem_id_b","text_norm":"text_b"})
semi_join = semi_join.dropna(subset=["text_a","text_b"])
semi_join["pair_type"] = "semi_hard_negative"
semi_join["label"] = 0
semi_join = semi_join[["pair_type","label","sub_a","sub_b","text_a","text_b","problem_id_a","problem_id_b","sim"]]
semi_join.to_parquet(f"{OUT_DIR}/pairs_semi_hard.parquet", compression="zstd")

# 6) ÏßëÍ≥Ñ & Í∑†Ìòï ÏÉòÌîåÎßÅ
pos_cnt  = con.execute(f"SELECT COUNT(*) FROM read_parquet('{OUT_DIR}/pairs_pos.parquet')").fetchone()[0]
hard_cnt = con.execute(f"SELECT COUNT(*) FROM read_parquet('{OUT_DIR}/pairs_hard.parquet')").fetchone()[0]
easy_cnt = con.execute(f"SELECT COUNT(*) FROM read_parquet('{OUT_DIR}/pairs_easy.parquet')").fetchone()[0]
semi_cnt = len(semi_join)
print(f"[CAND] pos={pos_cnt:,}, hard={hard_cnt:,}, easy={easy_cnt:,}, semi={semi_cnt:,}")

take_each = min(TARGET_PER_CLASS, pos_cnt, hard_cnt, easy_cnt, semi_cnt)
assert take_each > 0, "ÌõÑÎ≥¥Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§. cap/bucket/thresholdÎ•º ÏôÑÌôîÌïòÏÑ∏Ïöî."
print(f"[INFO] take_each per class = {take_each:,}")

# 7) ÏµúÏ¢Ö Î≥ëÌï© (Ïä§ÌÇ§Îßà ÌÜµÏùº 8Ïª¨Îüº, Î¨∏ÏûêÏó¥ ID ÎåÄÏùë)
con.execute(f"""
COPY (
  SELECT
    CAST(pair_type AS VARCHAR) AS pair_type,
    CAST(label AS INTEGER)     AS label,
    CAST(sub_a AS VARCHAR)     AS sub_a,
    CAST(sub_b AS VARCHAR)     AS sub_b,
    CAST(text_a AS VARCHAR)    AS text_a,
    CAST(text_b AS VARCHAR)    AS text_b,
    CAST(problem_id_a AS VARCHAR) AS problem_id_a,
    CAST(problem_id_b AS VARCHAR) AS problem_id_b
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_pos.parquet')  ORDER BY random() LIMIT {take_each}
  )
  UNION ALL
  SELECT
    CAST(pair_type AS VARCHAR), CAST(label AS INTEGER),
    CAST(sub_a AS VARCHAR), CAST(sub_b AS VARCHAR),
    CAST(text_a AS VARCHAR), CAST(text_b AS VARCHAR),
    CAST(problem_id_a AS VARCHAR), CAST(problem_id_b AS VARCHAR)
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_hard.parquet') ORDER BY random() LIMIT {take_each}
  )
  UNION ALL
  SELECT
    CAST(pair_type AS VARCHAR), CAST(label AS INTEGER),
    CAST(sub_a AS VARCHAR), CAST(sub_b AS VARCHAR),
    CAST(text_a AS VARCHAR), CAST(text_b AS VARCHAR),
    CAST(problem_id_a AS VARCHAR), CAST(problem_id_b AS VARCHAR)
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_easy.parquet') ORDER BY random() LIMIT {take_each}
  )
  UNION ALL
  SELECT
    CAST('semi_hard_negative' AS VARCHAR) AS pair_type,
    0                                      AS label,
    CAST(sub_a AS VARCHAR)                 AS sub_a,
    CAST(sub_b AS VARCHAR)                 AS sub_b,
    CAST(text_a AS VARCHAR)                AS text_a,
    CAST(text_b AS VARCHAR)                AS text_b,
    CAST(problem_id_a AS VARCHAR)          AS problem_id_a,
    CAST(problem_id_b AS VARCHAR)          AS problem_id_b
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_semi_hard.parquet') ORDER BY random() LIMIT {take_each}
  )
) TO '{OUT_DIR}/pairs_balanced_v4.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
""")

final_df = pd.read_parquet(f"{OUT_DIR}/pairs_balanced_v4.parquet")
print("‚úÖ Final balanced rows:", len(final_df))
print(final_df["pair_type"].value_counts())

import pandas as pd
P = "/content/drive/MyDrive/pairs_out_v4/pairs_balanced_v4.parquet"
df = pd.read_parquet(P)
print(df.columns.tolist())
print(df.isna().sum())
print(df.head(2)[["pair_type","label","problem_id_a","problem_id_b"]])

problems = df["problem_id_a"].unique()
import numpy as np
np.random.shuffle(problems)
train_ids, val_ids, test_ids = np.split(problems, [int(.8*len(problems)), int(.9*len(problems))])
train = df[df.problem_id_a.isin(train_ids)]
val   = df[df.problem_id_a.isin(val_ids)]
test  = df[df.problem_id_a.isin(test_ids)]

# ============================================================
# üìò Code Tokenizer Pipeline for Code Similarity Dataset
# ============================================================

!pip -q install datasets tokenizers transformers

# ------------------------------------------------------------
# 0) ÎùºÏù¥Î∏åÎü¨Î¶¨ & ÏÑ§Ï†ï
# ------------------------------------------------------------
import os, io, re, random, math, tokenize
from pathlib import Path
from typing import Iterable, Dict, Any, List, Tuple

import torch
from datasets import Dataset, DatasetDict
from tqdm.auto import tqdm

SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

# ------------------------------------------------------------
# ÎßàÏª§ / Î¶¨ÌÑ∞Îü¥ Ïä§ÌéòÏÖú ÌÜ†ÌÅ∞
# ------------------------------------------------------------
INDENT_TOK, DEDENT_TOK, NEWLINE_TOK = "<indent>", "<dedent>", "<newline>"
STR_TOK, NUM_TOK = "<str>", "<num>"
BASE_SPECIAL = ["<pad>", "<s>", "</s>", "<mask>", "<unk>", "<nl>", "<py>"]
ALL_SPECIAL = BASE_SPECIAL + [INDENT_TOK, DEDENT_TOK, NEWLINE_TOK, STR_TOK, NUM_TOK]

# ------------------------------------------------------------
# 1) ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò
# ------------------------------------------------------------
def code_to_marked_text(
    code: str,
    keep_comments: bool = False,
    normalize_literals: bool = False
) -> str:
    """
    - INDENT/DEDENT/NEWLINE ‚Üí Ïä§ÌéòÏÖú ÌÜ†ÌÅ∞ ÏÇΩÏûÖ
    - (ÏòµÏÖò) Ï£ºÏÑù Ï†úÍ±∞
    - (ÏòµÏÖò) Ïà´Ïûê/Î¨∏ÏûêÏó¥ Î¶¨ÌÑ∞Îü¥ÏùÑ <num>/<str>Î°ú ÏπòÌôò
    """
    code = code if isinstance(code, str) else str(code)
    out = []
    try:
        g = tokenize.generate_tokens(io.StringIO(code).readline)
        for tt, ts, *_ in g:
            if tt == tokenize.INDENT:
                out.append(f" {INDENT_TOK} ")
            elif tt == tokenize.DEDENT:
                out.append(f" {DEDENT_TOK} ")
            elif tt in (tokenize.NEWLINE, tokenize.NL):
                out.append(f" {NEWLINE_TOK} ")
            elif tt == tokenize.COMMENT and not keep_comments:
                continue
            elif normalize_literals and tt == tokenize.NUMBER:
                out.append(f" {NUM_TOK} ")
            elif normalize_literals and tt == tokenize.STRING:
                out.append(f" {STR_TOK} ")
            else:
                out.append(" " + ts + " ")
    except Exception:
        # Ïã§Ìå® Ïãú Ï†ÑÏ≤¥Î•º ÌÜµÏúºÎ°ú Ï≤òÎ¶¨
        s = code.replace("\r\n","\n").replace("\r","\n").replace("\n",f" {NEWLINE_TOK} ")
        out = [" " + s + " "]
    return " ".join("".join(out).split())

# ------------------------------------------------------------
# 2) HF DatasetDict ÎπåÎìú
# ------------------------------------------------------------
def build_dataset_from_df(df, text_col="text_a", val_ratio=0.1) -> DatasetDict:
    """
    df[text_col] ‚Üí Ï†ÑÏ≤òÎ¶¨(markers) ‚Üí HuggingFace DatasetDict(train/validation)
    """
    texts = [code_to_marked_text(s) for s in tqdm(df[text_col].astype(str), desc="marking")]
    ds = Dataset.from_dict({"text": texts})
    split = ds.train_test_split(test_size=val_ratio, seed=SEED)
    return DatasetDict(train=split["train"], validation=split["test"])

# ------------------------------------------------------------
# 3) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÌïôÏäµ (Unigram)
# ------------------------------------------------------------
from tokenizers import Tokenizer
from tokenizers.models import Unigram, BPE
from tokenizers.trainers import UnigramTrainer, BpeTrainer
from tokenizers.pre_tokenizers import Whitespace, ByteLevel
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from transformers import PreTrainedTokenizerFast

def train_tokenizer_from_ds(
    ds: DatasetDict,
    save_dir: str,
    kind: str = "unigram",
    vocab_size: int = 32000,
    max_piece_length: int = 16
) -> PreTrainedTokenizerFast:
    """
    - kind: "unigram" (Ï∂îÏ≤ú) or "bpe"
    - ds["train"]["text"] Í∏∞Ï§ÄÏúºÎ°ú ÌïôÏäµ
    """
    os.makedirs(save_dir, exist_ok=True)
    tmp_corpus = os.path.join(save_dir, "corpus_tmp.txt")
    with open(tmp_corpus, "w", encoding="utf-8") as f:
        for s in ds["train"]["text"]:
            if s: f.write(s.strip() + "\n")

    if kind == "unigram":
        tk = Tokenizer(Unigram())
        tk.pre_tokenizer = Whitespace()
        trainer = UnigramTrainer(
            vocab_size=vocab_size,
            special_tokens=ALL_SPECIAL,
            unk_token="<unk>",
            max_piece_length=max_piece_length
        )
    elif kind == "bpe":
        tk = Tokenizer(BPE(unk_token="<unk>"))
        tk.pre_tokenizer = ByteLevel()
        trainer = BpeTrainer(
            vocab_size=vocab_size,
            special_tokens=ALL_SPECIAL,
            min_frequency=2
        )
        tk.decoder = ByteLevelDecoder()
    else:
        raise ValueError("kind must be 'unigram' or 'bpe'")

    tk.train([tmp_corpus], trainer)
    tok_json = os.path.join(save_dir, f"{kind}.json")
    tk.save(tok_json)
    os.remove(tmp_corpus)  # ÏûÑÏãú corpus ÏÇ≠Ï†ú

    fast = PreTrainedTokenizerFast(
        tokenizer_file=tok_json,
        bos_token="<s>", eos_token="</s>",
        pad_token="<pad>", mask_token="<mask>", unk_token="<unk>"
    )
    fast.add_special_tokens({
        "additional_special_tokens": [
            t for t in ALL_SPECIAL if t not in fast.all_special_tokens
        ]
    })
    fast.save_pretrained(save_dir)
    print(f"‚úÖ Tokenizer saved to {save_dir}")
    return fast

# ------------------------------------------------------------
# 4) Ïã§Ìñâ (ÏÇ¨Ïö©Ïûê Îç∞Ïù¥ÌÑ∞ÏÖã)
# ------------------------------------------------------------
import pandas as pd

PAIR_PATH = "/content/drive/MyDrive/pairs_out/pairs_balanced_1to1to1.parquet"
SAVE_DIR = "/content/drive/MyDrive/tokenizer_out"

# üü¶ 1Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
df = pd.read_parquet(PAIR_PATH)
print("Loaded pairs:", len(df))
print(df.columns)

# üü¶ 2Îã®Í≥Ñ: DatasetDict ÏÉùÏÑ± (train/val)
ds = build_dataset_from_df(df, text_col="text_a", val_ratio=0.1)

# üü¶ 3Îã®Í≥Ñ: ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÌïôÏäµ (Unigram)
tok = train_tokenizer_from_ds(ds, SAVE_DIR, kind="unigram", vocab_size=32000)

# ÌôïÏù∏
print("Vocab size:", tok.vocab_size)
print("ÏòàÏãú ÌÜ†ÌÅ∞Ìôî Í≤∞Í≥º:", tok.encode(df.iloc[0]["text_a"]).tokens[:50])