# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c0wHoE8XSHrA5rmL6fukKHH7Iun57tAg
"""

from google.colab import drive
drive.mount('/content/drive')

!pip -q install pyarrow fastparquet duckdb polars

import pyarrow.parquet as pq

par_path = "/content/drive/MyDrive/code_corpus_processed.parquet"
pf = pq.ParquetFile(par_path)

# 스키마
print("Schema:\n", pf.schema)

# 행그룹 개수/행 수/열 수
print(f"\nNum row groups: {pf.num_row_groups}")
print(f"Num columns: {len(pf.schema)}")
try:
    # Parquet metadata에 총 row 수가 들어있으면 표시
    print("Total rows (from metadata):", pf.metadata.num_rows)
except:
    pass

# 각 행그룹당 대략 크기/행수
for i in range(pf.num_row_groups):
    rg = pf.metadata.row_group(i)
    print(f"RowGroup {i}: rows={rg.num_rows}, total_byte_size={rg.total_byte_size}")

# ============================================================
# Pair Build
#  - Positive:        same problem AC–AC
#  - Hard Negative:   same problem AC–WA/RE/TLE
#  - Easy Negative:   different problem AC–AC (random bucket)
#  - Semi-Hard Neg.:  different problem AC–AC (TF-IDF nearest in bucket)
#  - Final union with unified 8-column schema
# ============================================================

!pip -q install duckdb pyarrow pandas scikit-learn

import os, textwrap, duckdb, pandas as pd, numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors

# ----------------------------
# 사용자 설정
# ----------------------------
PAR_PATH = "/content/drive/MyDrive/code_corpus_processed.parquet"   # text_norm 포함된 전처리 Parquet
OUT_DIR  = "/content/drive/MyDrive/pairs_out_v4"
os.makedirs(OUT_DIR, exist_ok=True)

LANG_FILTER   = None            # "Python" or None
MIN_CODE_LEN  = 0
HARD_STATUSES = ["WA","RE","TLE"]

# 타깃 쌍 수
TARGET_TOTAL_PAIRS = 200_000
TARGET_PER_CLASS   = TARGET_TOTAL_PAIRS // 4  # pos/hard/easy/semi_hard 균형

# 스케일 파라미터
PER_PROBLEM_POS_CAP  = 50
PER_PROBLEM_HARD_CAP = 50
EASY_BUCKETS         = 1024
PER_BUCKET_EASY_CAP  = 80
SAMPLE_MULTIPLIER    = 4

# Semi-hard 마이닝
SEMI_BUCKETS              = 1024
SEMI_PER_BUCKET           = 40
SEMI_K_NEIGHBORS          = 6
SEMI_MIN_SIM              = 0.15
SEMI_MAX_TEXTS_PER_BUCKET = 1200  # 메모리 안전장치

# ----------------------------
# DuckDB 세션
# ----------------------------
con = duckdb.connect()
con.execute("PRAGMA threads=8;")
con.execute("PRAGMA temp_directory='/content';")

# 0) 로드
con.execute(f"CREATE OR REPLACE VIEW subs AS SELECT * FROM read_parquet('{PAR_PATH}');")

# 1) 정규화
con.execute(textwrap.dedent("""
CREATE OR REPLACE VIEW subs_norm AS
SELECT
  *,
  CASE
    WHEN lower(status) LIKE '%accept%'                         THEN 'AC'
    WHEN lower(status) LIKE '%wrong answer%'                   THEN 'WA'
    WHEN lower(status) LIKE '%presentation error%'             THEN 'WA'
    WHEN lower(status) LIKE '%time limit exceeded%' OR lower(status) LIKE 'tle%' THEN 'TLE'
    WHEN lower(status) LIKE '%runtime error%' OR lower(status) LIKE 'rte%'       THEN 'RE'
    WHEN lower(status) LIKE '%compile error%'  OR lower(status) LIKE '%syntax%'   THEN 'CE'
    WHEN lower(status) LIKE '%memory limit exceeded%'          THEN 'MLE'
    WHEN lower(status) LIKE '%output limit exceeded%'          THEN 'OLE'
    ELSE upper(status)
  END AS status_norm,
  CASE
    WHEN lower(language) LIKE '%python%' OR language LIKE 'Py%' THEN 'Python'
    ELSE language
  END AS language_norm
FROM subs;
"""))

# 2) 클린 필터
conds = ["text_norm IS NOT NULL", "text_norm_sha1 IS NOT NULL", f"length(text_norm) >= {MIN_CODE_LEN}"]
if LANG_FILTER:
    conds.append(f"language_norm = '{LANG_FILTER}'")
WHERE_CLEAN = " AND ".join(conds)

con.execute(f"CREATE OR REPLACE VIEW clean AS SELECT * FROM subs_norm WHERE {WHERE_CLEAN};")

# 3) AC/WA 분기 + 중복 제거
con.execute("CREATE OR REPLACE VIEW ac_raw AS SELECT * FROM clean WHERE status_norm='AC';")
hs = ",".join([f"'{s}'" for s in HARD_STATUSES])
con.execute(f"CREATE OR REPLACE VIEW wa_raw AS SELECT * FROM clean WHERE status_norm IN ({hs});")

con.execute("""
CREATE OR REPLACE VIEW ac AS
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY problem_id, text_norm_sha1 ORDER BY date) rn
  FROM ac_raw
) t WHERE rn=1;
""")
con.execute("""
CREATE OR REPLACE VIEW wa AS
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY problem_id, text_norm_sha1 ORDER BY date) rn
  FROM wa_raw
) t WHERE rn=1;
""")

# 4-A) Positive: same problem AC–AC
con.execute(f"""
CREATE OR REPLACE TEMP TABLE pos_ids AS
WITH ac_pick AS (
  SELECT submission_id, problem_id, text_norm_sha1,
         ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) AS rn
  FROM ac
)
SELECT a.submission_id AS sub_a,
       b.submission_id AS sub_b,
       a.problem_id    AS problem_id
FROM ac_pick a
JOIN ac_pick b
  ON a.problem_id=b.problem_id AND a.rn%2=1 AND b.rn=a.rn+1
WHERE a.text_norm_sha1<>b.text_norm_sha1
QUALIFY ROW_NUMBER() OVER (PARTITION BY a.problem_id ORDER BY random()) <= {PER_PROBLEM_POS_CAP};
""")

# 4-B) Hard: same problem AC–WA
con.execute(f"""
CREATE OR REPLACE TEMP TABLE hard_ids AS
WITH ac_pick AS (
  SELECT submission_id, problem_id, text_norm_sha1,
         ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) AS rn,
         COUNT(*) OVER (PARTITION BY problem_id) AS ac_n
  FROM ac
),
wa_pick AS (
  SELECT submission_id, problem_id, text_norm_sha1,
         ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) AS rn,
         COUNT(*) OVER (PARTITION BY problem_id) AS wa_n
  FROM wa
),
paired AS (
  SELECT a.submission_id AS sub_a, w.submission_id AS sub_b,
         a.problem_id, a.rn, a.ac_n, w.wa_n
  FROM ac_pick a
  JOIN wa_pick w ON a.problem_id=w.problem_id AND a.rn=w.rn
)
SELECT sub_a, sub_b, problem_id
FROM paired
QUALIFY rn <= LEAST(ac_n, wa_n)
AND ROW_NUMBER() OVER (PARTITION BY problem_id ORDER BY random()) <= {PER_PROBLEM_HARD_CAP};
""")

# 4-C) Easy: diff problem AC–AC (랜덤 버킷)
con.execute(f"""
CREATE OR REPLACE TEMP VIEW ac_bucket AS
SELECT *,
       (abs(hash(text_norm_sha1)) % {EASY_BUCKETS}) AS bkt
FROM ac;
""")
con.execute(f"""
CREATE OR REPLACE TEMP TABLE easy_ids AS
WITH sampled AS (
  SELECT submission_id, problem_id, text_norm_sha1, bkt
  FROM ac_bucket
  QUALIFY ROW_NUMBER() OVER (PARTITION BY bkt ORDER BY random()) <= {PER_BUCKET_EASY_CAP} * {SAMPLE_MULTIPLIER}
),
s1 AS (
  SELECT submission_id, problem_id, text_norm_sha1, bkt,
         ROW_NUMBER() OVER (PARTITION BY bkt ORDER BY random()) AS rn
  FROM sampled
),
s2 AS (
  SELECT submission_id, problem_id, text_norm_sha1, bkt,
         ROW_NUMBER() OVER (PARTITION BY bkt ORDER BY random()) AS rn
  FROM sampled
)
SELECT a.submission_id AS sub_a,
       b.submission_id AS sub_b,
       a.bkt AS bkt
FROM s1 a
JOIN s2 b
  ON a.bkt=b.bkt AND a.rn=b.rn
WHERE a.problem_id<>b.problem_id
  AND a.text_norm_sha1<>b.text_norm_sha1
QUALIFY ROW_NUMBER() OVER (PARTITION BY a.bkt ORDER BY random()) <= {PER_BUCKET_EASY_CAP};
""")

# 4-D) Semi-Hard: diff problem AC–AC (TF-IDF 근사 최근접 within bucket)
ac_bucket_df = con.execute(f"""
SELECT submission_id, problem_id, text_norm, (abs(hash(text_norm_sha1)) % {SEMI_BUCKETS}) AS bkt
FROM ac
""").df()

semi_pairs = []
rng = np.random.default_rng(0)
for bkt, grp in ac_bucket_df.groupby("bkt"):
    if len(grp) < 3:
        continue
    if len(grp) > SEMI_MAX_TEXTS_PER_BUCKET:
        grp = grp.sample(SEMI_MAX_TEXTS_PER_BUCKET, random_state=0)
    texts = grp["text_norm"].astype(str).tolist()
    sids  = grp["submission_id"].tolist()
    pids  = grp["problem_id"].tolist()

    vec = TfidfVectorizer(analyzer="char", ngram_range=(3,5), min_df=2)
    X = vec.fit_transform(texts)
    if X.shape[0] < 3:
        continue

    nn = NearestNeighbors(n_neighbors=min(SEMI_K_NEIGHBORS+1, len(texts)), metric="cosine", n_jobs=-1)
    nn.fit(X)
    dists, idxs = nn.kneighbors(X, return_distance=True)

    per_bucket = 0
    for i in range(len(texts)):
        if per_bucket >= SEMI_PER_BUCKET: break
        for jpos in range(1, idxs.shape[1]):
            j = idxs[i, jpos]
            if pids[i] == pids[j]:  # 같은 problem 제외
                continue
            sim = 1.0 - dists[i, jpos]
            if sim < SEMI_MIN_SIM:
                continue
            a, b = (i, j) if sids[i] < sids[j] else (j, i)
            semi_pairs.append((sids[a], sids[b], bkt, sim))
            per_bucket += 1
            if per_bucket >= SEMI_PER_BUCKET: break

semi_df = pd.DataFrame(semi_pairs, columns=["sub_a","sub_b","bkt","sim"]).drop_duplicates(["sub_a","sub_b"])
print("Semi-hard pairs (final):", len(semi_df))

# 5) 클래스별 파일 생성 (여기서 text_a/text_b는 text_norm = 정규화 코드)
def copy_parquet(sql, path):
    con.execute(f"COPY ({sql}) TO '{path}' (FORMAT PARQUET, COMPRESSION ZSTD);")

# Positive (AC–AC, same problem)
copy_parquet(f"""
  SELECT
    'positive' AS pair_type, 1 AS label,
    pa.sub_a, pa.sub_b,
    a.text_norm AS text_a, b.text_norm AS text_b,   -- ✅ 정규화 코드 사용
    a.problem_id AS problem_id_a, b.problem_id AS problem_id_b
  FROM (SELECT sub_a, sub_b FROM pos_ids) pa
  JOIN ac a ON pa.sub_a=a.submission_id
  JOIN ac b ON pa.sub_b=b.submission_id
""", f"{OUT_DIR}/pairs_pos.parquet")

# Hard (AC–WA, same problem)
copy_parquet(f"""
  SELECT
    'hard_negative' AS pair_type, 0 AS label,
    ha.sub_a, ha.sub_b,
    a.text_norm AS text_a, w.text_norm AS text_b,   -- ✅ 정규화 코드 사용
    a.problem_id AS problem_id_a, w.problem_id AS problem_id_b
  FROM (SELECT sub_a, sub_b FROM hard_ids) ha
  JOIN ac a ON ha.sub_a=a.submission_id
  JOIN wa w ON ha.sub_b=w.submission_id
""", f"{OUT_DIR}/pairs_hard.parquet")

# Easy (AC–AC, diff problem)
copy_parquet(f"""
  SELECT
    'easy_negative' AS pair_type, 0 AS label,
    ea.sub_a, ea.sub_b,
    a.text_norm AS text_a, b.text_norm AS text_b,   -- ✅ 정규화 코드 사용
    a.problem_id AS problem_id_a, b.problem_id AS problem_id_b
  FROM (SELECT sub_a, sub_b FROM easy_ids) ea
  JOIN ac a ON ea.sub_a=a.submission_id
  JOIN ac b ON ea.sub_b=b.submission_id
""", f"{OUT_DIR}/pairs_easy.parquet")

# Semi-hard (pandas join)
ac_table = con.execute("SELECT submission_id, problem_id, text_norm FROM ac").df().set_index("submission_id")
semi_join = semi_df.join(ac_table, on="sub_a").rename(columns={"problem_id":"problem_id_a","text_norm":"text_a"})
semi_join = semi_join.join(ac_table, on="sub_b").rename(columns={"problem_id":"problem_id_b","text_norm":"text_b"})
semi_join = semi_join.dropna(subset=["text_a","text_b"])
semi_join["pair_type"] = "semi_hard_negative"
semi_join["label"] = 0
semi_join = semi_join[["pair_type","label","sub_a","sub_b","text_a","text_b","problem_id_a","problem_id_b","sim"]]
semi_join.to_parquet(f"{OUT_DIR}/pairs_semi_hard.parquet", compression="zstd")

# 6) 집계 & 균형 샘플링
pos_cnt  = con.execute(f"SELECT COUNT(*) FROM read_parquet('{OUT_DIR}/pairs_pos.parquet')").fetchone()[0]
hard_cnt = con.execute(f"SELECT COUNT(*) FROM read_parquet('{OUT_DIR}/pairs_hard.parquet')").fetchone()[0]
easy_cnt = con.execute(f"SELECT COUNT(*) FROM read_parquet('{OUT_DIR}/pairs_easy.parquet')").fetchone()[0]
semi_cnt = len(semi_join)
print(f"[CAND] pos={pos_cnt:,}, hard={hard_cnt:,}, easy={easy_cnt:,}, semi={semi_cnt:,}")

take_each = min(TARGET_PER_CLASS, pos_cnt, hard_cnt, easy_cnt, semi_cnt)
assert take_each > 0, "후보가 부족합니다. cap/bucket/threshold를 완화하세요."
print(f"[INFO] take_each per class = {take_each:,}")

# 7) 최종 병합 (스키마 통일 8컬럼, 문자열 ID 대응)
con.execute(f"""
COPY (
  SELECT
    CAST(pair_type AS VARCHAR) AS pair_type,
    CAST(label AS INTEGER)     AS label,
    CAST(sub_a AS VARCHAR)     AS sub_a,
    CAST(sub_b AS VARCHAR)     AS sub_b,
    CAST(text_a AS VARCHAR)    AS text_a,
    CAST(text_b AS VARCHAR)    AS text_b,
    CAST(problem_id_a AS VARCHAR) AS problem_id_a,
    CAST(problem_id_b AS VARCHAR) AS problem_id_b
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_pos.parquet')  ORDER BY random() LIMIT {take_each}
  )
  UNION ALL
  SELECT
    CAST(pair_type AS VARCHAR), CAST(label AS INTEGER),
    CAST(sub_a AS VARCHAR), CAST(sub_b AS VARCHAR),
    CAST(text_a AS VARCHAR), CAST(text_b AS VARCHAR),
    CAST(problem_id_a AS VARCHAR), CAST(problem_id_b AS VARCHAR)
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_hard.parquet') ORDER BY random() LIMIT {take_each}
  )
  UNION ALL
  SELECT
    CAST(pair_type AS VARCHAR), CAST(label AS INTEGER),
    CAST(sub_a AS VARCHAR), CAST(sub_b AS VARCHAR),
    CAST(text_a AS VARCHAR), CAST(text_b AS VARCHAR),
    CAST(problem_id_a AS VARCHAR), CAST(problem_id_b AS VARCHAR)
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_easy.parquet') ORDER BY random() LIMIT {take_each}
  )
  UNION ALL
  SELECT
    CAST('semi_hard_negative' AS VARCHAR) AS pair_type,
    0                                      AS label,
    CAST(sub_a AS VARCHAR)                 AS sub_a,
    CAST(sub_b AS VARCHAR)                 AS sub_b,
    CAST(text_a AS VARCHAR)                AS text_a,
    CAST(text_b AS VARCHAR)                AS text_b,
    CAST(problem_id_a AS VARCHAR)          AS problem_id_a,
    CAST(problem_id_b AS VARCHAR)          AS problem_id_b
  FROM (
    SELECT * FROM read_parquet('{OUT_DIR}/pairs_semi_hard.parquet') ORDER BY random() LIMIT {take_each}
  )
) TO '{OUT_DIR}/pairs_balanced_v4.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);
""")

final_df = pd.read_parquet(f"{OUT_DIR}/pairs_balanced_v4.parquet")
print("✅ Final balanced rows:", len(final_df))
print(final_df["pair_type"].value_counts())

import pandas as pd
P = "/content/drive/MyDrive/pairs_out_v4/pairs_balanced_v4.parquet"
df = pd.read_parquet(P)
print(df.columns.tolist())
print(df.isna().sum())
print(df.head(2)[["pair_type","label","problem_id_a","problem_id_b"]])

problems = df["problem_id_a"].unique()
import numpy as np
np.random.shuffle(problems)
train_ids, val_ids, test_ids = np.split(problems, [int(.8*len(problems)), int(.9*len(problems))])
train = df[df.problem_id_a.isin(train_ids)]
val   = df[df.problem_id_a.isin(val_ids)]
test  = df[df.problem_id_a.isin(test_ids)]

# ============================================================
# 📘 Code Tokenizer Pipeline for Code Similarity Dataset
# ============================================================

!pip -q install datasets tokenizers transformers

# ------------------------------------------------------------
# 0) 라이브러리 & 설정
# ------------------------------------------------------------
import os, io, re, random, math, tokenize
from pathlib import Path
from typing import Iterable, Dict, Any, List, Tuple

import torch
from datasets import Dataset, DatasetDict
from tqdm.auto import tqdm

SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

# ------------------------------------------------------------
# 마커 / 리터럴 스페셜 토큰
# ------------------------------------------------------------
INDENT_TOK, DEDENT_TOK, NEWLINE_TOK = "<indent>", "<dedent>", "<newline>"
STR_TOK, NUM_TOK = "<str>", "<num>"
BASE_SPECIAL = ["<pad>", "<s>", "</s>", "<mask>", "<unk>", "<nl>", "<py>"]
ALL_SPECIAL = BASE_SPECIAL + [INDENT_TOK, DEDENT_TOK, NEWLINE_TOK, STR_TOK, NUM_TOK]

# ------------------------------------------------------------
# 1) 텍스트 전처리 함수
# ------------------------------------------------------------
def code_to_marked_text(
    code: str,
    keep_comments: bool = False,
    normalize_literals: bool = False
) -> str:
    """
    - INDENT/DEDENT/NEWLINE → 스페셜 토큰 삽입
    - (옵션) 주석 제거
    - (옵션) 숫자/문자열 리터럴을 <num>/<str>로 치환
    """
    code = code if isinstance(code, str) else str(code)
    out = []
    try:
        g = tokenize.generate_tokens(io.StringIO(code).readline)
        for tt, ts, *_ in g:
            if tt == tokenize.INDENT:
                out.append(f" {INDENT_TOK} ")
            elif tt == tokenize.DEDENT:
                out.append(f" {DEDENT_TOK} ")
            elif tt in (tokenize.NEWLINE, tokenize.NL):
                out.append(f" {NEWLINE_TOK} ")
            elif tt == tokenize.COMMENT and not keep_comments:
                continue
            elif normalize_literals and tt == tokenize.NUMBER:
                out.append(f" {NUM_TOK} ")
            elif normalize_literals and tt == tokenize.STRING:
                out.append(f" {STR_TOK} ")
            else:
                out.append(" " + ts + " ")
    except Exception:
        # 실패 시 전체를 통으로 처리
        s = code.replace("\r\n","\n").replace("\r","\n").replace("\n",f" {NEWLINE_TOK} ")
        out = [" " + s + " "]
    return " ".join("".join(out).split())

# ------------------------------------------------------------
# 2) HF DatasetDict 빌드
# ------------------------------------------------------------
def build_dataset_from_df(df, text_col="text_a", val_ratio=0.1) -> DatasetDict:
    """
    df[text_col] → 전처리(markers) → HuggingFace DatasetDict(train/validation)
    """
    texts = [code_to_marked_text(s) for s in tqdm(df[text_col].astype(str), desc="marking")]
    ds = Dataset.from_dict({"text": texts})
    split = ds.train_test_split(test_size=val_ratio, seed=SEED)
    return DatasetDict(train=split["train"], validation=split["test"])

# ------------------------------------------------------------
# 3) 토크나이저 학습 (Unigram)
# ------------------------------------------------------------
from tokenizers import Tokenizer
from tokenizers.models import Unigram, BPE
from tokenizers.trainers import UnigramTrainer, BpeTrainer
from tokenizers.pre_tokenizers import Whitespace, ByteLevel
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from transformers import PreTrainedTokenizerFast

def train_tokenizer_from_ds(
    ds: DatasetDict,
    save_dir: str,
    kind: str = "unigram",
    vocab_size: int = 32000,
    max_piece_length: int = 16
) -> PreTrainedTokenizerFast:
    """
    - kind: "unigram" (추천) or "bpe"
    - ds["train"]["text"] 기준으로 학습
    """
    os.makedirs(save_dir, exist_ok=True)
    tmp_corpus = os.path.join(save_dir, "corpus_tmp.txt")
    with open(tmp_corpus, "w", encoding="utf-8") as f:
        for s in ds["train"]["text"]:
            if s: f.write(s.strip() + "\n")

    if kind == "unigram":
        tk = Tokenizer(Unigram())
        tk.pre_tokenizer = Whitespace()
        trainer = UnigramTrainer(
            vocab_size=vocab_size,
            special_tokens=ALL_SPECIAL,
            unk_token="<unk>",
            max_piece_length=max_piece_length
        )
    elif kind == "bpe":
        tk = Tokenizer(BPE(unk_token="<unk>"))
        tk.pre_tokenizer = ByteLevel()
        trainer = BpeTrainer(
            vocab_size=vocab_size,
            special_tokens=ALL_SPECIAL,
            min_frequency=2
        )
        tk.decoder = ByteLevelDecoder()
    else:
        raise ValueError("kind must be 'unigram' or 'bpe'")

    tk.train([tmp_corpus], trainer)
    tok_json = os.path.join(save_dir, f"{kind}.json")
    tk.save(tok_json)
    os.remove(tmp_corpus)  # 임시 corpus 삭제

    fast = PreTrainedTokenizerFast(
        tokenizer_file=tok_json,
        bos_token="<s>", eos_token="</s>",
        pad_token="<pad>", mask_token="<mask>", unk_token="<unk>"
    )
    fast.add_special_tokens({
        "additional_special_tokens": [
            t for t in ALL_SPECIAL if t not in fast.all_special_tokens
        ]
    })
    fast.save_pretrained(save_dir)
    print(f"✅ Tokenizer saved to {save_dir}")
    return fast

# ------------------------------------------------------------
# 4) 실행 (사용자 데이터셋)
# ------------------------------------------------------------
import pandas as pd

PAIR_PATH = "/content/drive/MyDrive/pairs_out/pairs_balanced_1to1to1.parquet"
SAVE_DIR = "/content/drive/MyDrive/tokenizer_out"

# 🟦 1단계: 데이터 로드
df = pd.read_parquet(PAIR_PATH)
print("Loaded pairs:", len(df))
print(df.columns)

# 🟦 2단계: DatasetDict 생성 (train/val)
ds = build_dataset_from_df(df, text_col="text_a", val_ratio=0.1)

# 🟦 3단계: 토크나이저 학습 (Unigram)
tok = train_tokenizer_from_ds(ds, SAVE_DIR, kind="unigram", vocab_size=32000)

# 확인
print("Vocab size:", tok.vocab_size)
print("예시 토큰화 결과:", tok.encode(df.iloc[0]["text_a"]).tokens[:50])