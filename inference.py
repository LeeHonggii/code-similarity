# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14tHiobH8lf3-8lrTBJWjnjoYSquv6Dox
"""

from google.colab import drive
drive.mount('/content/drive')

# ==========================================
# 자동 아키텍처 감지 + 안전 로더 (ckpt→모델 하이퍼 맞추기)
# ==========================================
import re
import torch
import torch.nn as nn

def _infer_arch_from_ckpt(sd):
    """
    ckpt state_dict에서 대략의 D_MODEL, N_LAYER 추론
    - D_MODEL: enc.tok.weight.shape[1] (임베딩 차원)
    - N_LAYER: enc.blocks.<k>. 로 시작하는 인덱스의 최댓값+1
    """
    d_model = None
    n_layer = None

    # 1) 임베딩에서 D_MODEL 추정
    for k, v in sd.items():
        if k.endswith("enc.tok.weight") and v.ndim == 2:
            d_model = v.shape[1]
            break
    # 보조: qkv.weight 모양(3D, D)에서도 추정 가능
    if d_model is None:
        for k, v in sd.items():
            if re.search(r"enc\.blocks\.\d+\.sa\.qkv\.weight$", k) and v.ndim == 2:
                d_model = v.shape[1]
                break

    # 2) 레이어 수 추정
    blk_idx = []
    for k in sd.keys():
        m = re.match(r"enc\.blocks\.(\d+)\.", k)
        if m:
            blk_idx.append(int(m.group(1)))
    if blk_idx:
        n_layer = max(blk_idx) + 1

    return d_model, n_layer

def _strip_prefix_and_normalize(sd):
    """module./model. 프리픽스 제거 + enc.* 네임스페이스로 정규화"""
    def strip_prefix(k):
        for p in ["module.", "model."]:
            if k.startswith(p): return k[len(p):]
        return k
    new_sd = {}
    for k, v in sd.items():
        k0 = strip_prefix(k)
        for old_enc in ["siamese.enc.", "base.enc.", "encoder.", "model.enc."]:
            if k0.startswith(old_enc):
                k0 = "enc." + k0[len(old_enc):]
                break
        new_sd[k0] = v
    return new_sd

def _safe_load_state_dict(model, sd):
    """
    - 임베딩 크기 불일치: 부분복사
    - 그 외 크기 불일치: 해당 키 건너뜀
    """
    # 1) 임베딩 처리
    model_embed_name, model_embed_param = None, None
    for name, p in model.named_parameters():
        if name.endswith("tok.weight"):
            model_embed_name, model_embed_param = name, p
            break
    ckpt_embed_key = None
    for k in sd.keys():
        if k.endswith("tok.weight"):
            ckpt_embed_key = k; break

    if model_embed_param is not None and ckpt_embed_key is not None:
        Wm = model_embed_param
        Wc = sd[ckpt_embed_key]
        if Wm.shape != Wc.shape:
            print(f"[auto-load] embedding mismatch: ckpt {tuple(Wc.shape)} vs model {tuple(Wm.shape)} → partial copy")
            with torch.no_grad():
                Wm.data.zero_()
                n = min(Wc.shape[0], Wm.shape[0])
                d = min(Wc.shape[1], Wm.shape[1])
                Wm.data[:n, :d].copy_(Wc[:n, :d])
            # 임베딩은 이미 주입했으므로 sd에서는 제거
            del sd[ckpt_embed_key]

    # 2) 나머지 텐서들: shape mismatch는 건너뜀
    pruned_sd = {}
    for k, v in sd.items():
        # 모델에 같은 이름 파라미터/버퍼가 있는지 확인
        tgt = dict(model.named_parameters())
        tgt.update(dict(model.named_buffers()))
        if k in tgt:
            if tgt[k].shape == v.shape:
                pruned_sd[k] = v
            else:
                # 정보 출력만 하고 skip
                print(f"[auto-load] skip shape mismatch: {k}: ckpt {tuple(v.shape)} vs model {tuple(tgt[k].shape)}")
        else:
            # 모델에 없는 키는 무시 (unexpected)
            pass

    missing, unexpected = model.load_state_dict(pruned_sd, strict=False)
    print(f"[auto-load] loaded (strict=False) → kept={len(pruned_sd)}  missing={len(missing)}  unexpected={len(unexpected)}")
    if missing:
        print("  missing (새로 추가된 모듈이거나 건너뛴 가중치):", missing[:10], "...")
    if unexpected:
        print("  unexpected (모델에 없는 키):", unexpected[:10], "...")
    return missing, unexpected

def build_and_load_fusion_auto(ckpt_path, vocab_size, pad_id):
    """
    1) ckpt 불러와 state_dict 정규화
    2) ckpt에서 D_MODEL/N_LAYER 추정 → FusionSiamese를 그 값으로 재생성
    3) 안전 로드(부분복사/shape mismatch skip)
    """
    sd = torch.load(ckpt_path, map_location="cpu")
    sd = sd.get("state_dict", sd)
    sd = _strip_prefix_and_normalize(sd)

    # 1) 아키텍처 자동 추정
    d_model_ckpt, n_layer_ckpt = _infer_arch_from_ckpt(sd)
    d_model_final = d_model_ckpt or D_MODEL
    n_layer_final = n_layer_ckpt or N_LAYER
    print(f"[auto-arch] inferred D_MODEL={d_model_final}, N_LAYER={n_layer_final}")

    # 2) 추정값으로 텍스트 인코더 재구성
    #    (FusionSiamese 내부에서 CodeEncoder를 사용할 때 이 값을 사용하도록 패치)
    class _PatchedCodeEncoder(CodeEncoder.__class__):
        pass  # placeholder (그대로 사용)

    # 전역 상수를 런타임 덮어쓰기 (현재 세션에서만 유효)
    globals()["D_MODEL"]  = d_model_final
    globals()["N_LAYER"]  = n_layer_final
    globals()["FF_DIM"]   = 4 * d_model_final

    # 3) 모델 생성 및 안전 로드
    model = FusionSiamese(vocab_size=vocab_size).to(DEVICE)
    _safe_load_state_dict(model, sd)
    return model

# =========================
# PATCH: ckpt에서 d_model/n_layer 추정 → 그 값으로 Fusion 생성
#       + 안전 로드(임베딩 부분복사, shape mismatch skip)
# =========================
import re, torch, torch.nn as nn
import torch.nn.functional as F

# 0) 현재 세션에 ASTMiniGNN, CodeEncoder 가 정의돼 있어야 합니다.
#    (위에서 이미 정의된 걸 그대로 사용)

class FusionSiameseParam(nn.Module):
    """파라미터화된 Fusion: d_model/n_layer/ast_out을 명시적으로 지정"""
    def __init__(self, vocab_size, d_model, n_layer, n_head=None, ast_out=128, pad_id=0, p_drop=0.1):
        super().__init__()
        # n_head가 None이면 d_model과 호환되는 기본값으로 (8 or 16 등) 맞춤
        if n_head is None:
            n_head = 8 if d_model % 8 == 0 else 4
        # CodeEncoder를 지정된 d_model/n_layer로 생성
        self.enc = CodeEncoder(
            vocab_size=vocab_size,
            d_model=d_model,
            n_head=n_head,
            n_layer=n_layer,
            p_drop=p_drop,
            pad_id=pad_id
        )
        # AST 분기 (out 차원은 ast_out과 정확히 맞춰야 concat이 일치)
        self.ast = ASTMiniGNN(out=ast_out)
        self.proj = nn.Linear(d_model + ast_out, d_model)
        # tau 그대로
        tau_init = 0.1
        self.tau  = nn.Parameter(torch.tensor(tau_init, dtype=torch.float32), requires_grad=True)

    def fuse(self, ids, mask, raw_codes, device):
        z_txt = self.enc(ids, mask)                      # [B, d_model]
        z_ast = self.ast.encode_batch(raw_codes, device) # [B, ast_out]
        z = torch.cat([z_txt, z_ast], dim=1)             # [B, d_model+ast_out]
        return F.normalize(self.proj(z), dim=-1)

    def forward(self, ids_a, mask_a, ids_b, mask_b, raw_a, raw_b):
        device = ids_a.device
        za = self.fuse(ids_a, mask_a, raw_a, device)
        zb = self.fuse(ids_b, mask_b, raw_b, device)
        return za, zb

# ---- 유틸: 프리픽스 정리 + enc.* 네임스페이스 정규화
def _strip_prefix_and_normalize(sd):
    def strip_prefix(k):
        for p in ["module.", "model."]:
            if k.startswith(p): return k[len(p):]
        return k
    new_sd = {}
    for k, v in sd.items():
        k0 = strip_prefix(k)
        for old_enc in ["siamese.enc.", "base.enc.", "encoder.", "model.enc."]:
            if k0.startswith(old_enc):
                k0 = "enc." + k0[len(old_enc):]
                break
        new_sd[k0] = v
    return new_sd

# ---- ckpt에서 d_model/n_layer 추정
def _infer_arch_from_ckpt(sd):
    d_model = None
    n_layer = None
    # 임베딩에서 d_model 추정
    for k, v in sd.items():
        if k.endswith("enc.tok.weight") and v.ndim == 2:
            d_model = v.shape[1]; break
    # 블록 개수에서 n_layer 추정
    idxs = []
    for k in sd.keys():
        m = re.match(r"enc\.blocks\.(\d+)\.", k)
        if m: idxs.append(int(m.group(1)))
    if idxs:
        n_layer = max(idxs) + 1
    return d_model, n_layer

# ---- 안전 로더: 임베딩 부분복사 + 나머지 shape mismatch는 skip
def _safe_load_state_dict(model, sd):
    # 임베딩 부분복사
    tgt_params = dict(model.named_parameters())
    tgt_params.update(dict(model.named_buffers()))
    # 현재 임베딩 키
    model_embed_name = next((n for n in tgt_params if n.endswith("tok.weight")), None)
    # ckpt 임베딩 키
    ckpt_embed_key   = next((k for k in sd.keys() if k.endswith("tok.weight")), None)

    if model_embed_name and ckpt_embed_key:
        Wm = tgt_params[model_embed_name]
        Wc = sd[ckpt_embed_key]
        if Wm.shape != Wc.shape:
            print(f"[auto-load] embedding mismatch: ckpt {tuple(Wc.shape)} vs model {tuple(Wm.shape)} → partial copy")
            with torch.no_grad():
                Wm.zero_()
                n = min(Wc.shape[0], Wm.shape[0])
                d = min(Wc.shape[1], Wm.shape[1])
                Wm[:n, :d].copy_(Wc[:n, :d])
            del sd[ckpt_embed_key]

    # 나머지 텐서들: shape mismatch는 skip
    pruned = {}
    for k, v in sd.items():
        if k in tgt_params and tgt_params[k].shape == v.shape:
            pruned[k] = v
        else:
            # 필요시 디버그 출력:
            # print(f"[auto-load] skip {k}: ckpt {getattr(v, 'shape', None)} vs model {getattr(tgt_params.get(k,''),'shape', None)}")
            pass
    missing, unexpected = model.load_state_dict(pruned, strict=False)
    print(f"[auto-load] loaded: kept={len(pruned)}  missing={len(missing)}  unexpected={len(unexpected)}")
    if missing:    print("  missing:", missing[:10], "...")
    if unexpected: print("  unexpected:", unexpected[:10], "...")
    return missing, unexpected

# ---- 최종: ckpt 기반으로 모델 생성 + 안전 로드
def build_and_load_fusion_auto(ckpt_path, vocab_size, pad_id=0, ast_out=128, p_drop=0.1):
    sd = torch.load(ckpt_path, map_location="cpu")
    sd = sd.get("state_dict", sd)
    sd = _strip_prefix_and_normalize(sd)

    d_model_ckpt, n_layer_ckpt = _infer_arch_from_ckpt(sd)
    if d_model_ckpt is None:
        raise RuntimeError("ckpt에서 d_model을 추정할 수 없습니다. enc.tok.weight 또는 qkv.weight를 확인하세요.")
    if n_layer_ckpt is None:
        # 기본값 6 레이어로 (필요시 바꿔도 OK)
        n_layer_ckpt = 6

    print(f"[auto-arch] inferred d_model={d_model_ckpt}, n_layer={n_layer_ckpt}")

    model = FusionSiameseParam(
        vocab_size=vocab_size,
        d_model=d_model_ckpt,
        n_layer=n_layer_ckpt,
        ast_out=ast_out,
        pad_id=pad_id,
        p_drop=p_drop
    ).to(DEVICE)

    _safe_load_state_dict(model, sd)
    return model

# ==========================================
# Dacon submission (AST-GNN Fusion + 스마트 로더)
# - 입력 test.csv: [pair_id, code1, code2]
# - 출력 submission.csv: [pair_id, similar]
# ==========================================
import os, io, tokenize, math, time
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import PreTrainedTokenizerFast
from tqdm.auto import tqdm
from einops import rearrange

TEST_CSV      = "/content/drive/MyDrive/test_norm_final.csv"
TOKENIZER_DIR = "/content/drive/MyDrive/tokenizer_out"
FINETUNE_CKPT = "/content/drive/MyDrive/code_similarity_from_scratch_v4/finetune_astgnn_best.pt"
SUBMIT_PATH   = "/content/drive/MyDrive/submission.csv"
MAX_LEN       = 512
BATCH_SIZE    = 64

# ===== 1) 환경 =====
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("DEVICE:", DEVICE)

# ===== 2) 토크나이저 & vocab =====
tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_DIR)
PAD_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
VOCAB_SIZE = getattr(tokenizer, "vocab_size", None) or len(tokenizer)
print(f"PAD_ID={PAD_ID}, VOCAB_SIZE={VOCAB_SIZE}")

# ===== 3) 전처리 함수(사전학습과 동일) =====
def code_to_marked_text(code: str) -> str:
    INDENT_TOK, DEDENT_TOK, NEWLINE_TOK = "<indent>", "<dedent>", "<newline>"
    out = []
    try:
        g = tokenize.generate_tokens(io.StringIO(code).readline)
        for tt, ts, *_ in g:
            if tt == tokenize.INDENT: out.append(f" {INDENT_TOK} ")
            elif tt == tokenize.DEDENT: out.append(f" {DEDENT_TOK} ")
            elif tt in (tokenize.NEWLINE, tokenize.NL): out.append(f" {NEWLINE_TOK} ")
            elif tt == tokenize.COMMENT: continue
            else: out.append(" " + ts + " ")
    except Exception:
        out = [" " + str(code).replace("\n", f" {NEWLINE_TOK} ") + " "]
    return " ".join("".join(out).split())

# ===== 4) 모델 정의(사전학습과 동일 구조 유지) =====
D_MODEL, N_HEAD, N_LAYER, FF_DIM, DROPOUT = 256, 8, 6, 4*256, 0.1
USE_ABS_POS = False  # 사전학습과 다르면 ckpt 불일치
USE_ROPE = False
USE_ALIBI = False
TAU_INIT, TAU_LEARNABLE = 0.1, True

class MultiheadSelfAttention(nn.Module):
    def __init__(self, d_model, n_head, dropout=0.1, rope=False, alibi=False):
        super().__init__()
        assert not (rope and alibi)
        assert d_model % n_head == 0
        self.nh = n_head
        self.qkv = nn.Linear(d_model, 3*d_model)
        self.o   = nn.Linear(d_model, d_model)
        self.drop= nn.Dropout(dropout)
        self.rope, self.alibi = rope, alibi
    def forward(self, x, key_pad_mask):
        B,L,D = x.shape
        q,k,v = self.qkv(x).chunk(3, dim=-1)
        q = rearrange(q, 'b l (h d) -> b h l d', h=self.nh)
        k = rearrange(k, 'b l (h d) -> b h l d', h=self.nh)
        v = rearrange(v, 'b l (h d) -> b h l d', h=self.nh)
        att = torch.einsum('b h i d, b h j d -> b h i j', q, k) / (q.shape[-1] ** 0.5)
        if key_pad_mask is not None:
            att = att.masked_fill(key_pad_mask[:, None, None, :], float('-inf'))
        w = torch.softmax(att, dim=-1)
        h = torch.einsum('b h i j, b h j d -> b h i d', w, v)
        h = rearrange(h, 'b h l d -> b l (h d)')
        return self.o(h)

class EncoderBlock(nn.Module):
    def __init__(self, d_model, n_head, p=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.sa  = MultiheadSelfAttention(d_model, n_head, p)
        self.drop= nn.Dropout(p)
        self.ln2 = nn.LayerNorm(d_model)
        self.ff  = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.GELU(), nn.Dropout(p), nn.Linear(4*d_model, d_model))
    def forward(self, x, key_pad_mask):
        x = x + self.drop(self.sa(self.ln1(x), key_pad_mask))
        x = x + self.drop(self.ff(self.ln2(x)))
        return x

class CodeEncoder(nn.Module):
    def __init__(self, vocab_size, d_model=D_MODEL, n_head=N_HEAD, n_layer=N_LAYER, pad_id=PAD_ID, p_drop=DROPOUT):
        super().__init__()
        self.tok = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
        self.blocks = nn.ModuleList([EncoderBlock(d_model, n_head, p_drop) for _ in range(n_layer)])
        self.ln = nn.LayerNorm(d_model)
    def forward(self, x, attn_mask):
        B,L = x.size()
        h = self.tok(x)
        key_pad_mask = (attn_mask == 0)
        for blk in self.blocks: h = blk(h, key_pad_mask)
        h = self.ln(h)
        denom = attn_mask.sum(dim=1).clamp(min=1).unsqueeze(-1)
        pooled = (h * attn_mask.unsqueeze(-1)).sum(dim=1) / denom
        return F.normalize(pooled, dim=-1)

# --- Shallow AST-GNN ---
import ast as _pyast
class ASTMiniGNN(nn.Module):
    def __init__(self, type_vocab_size=256, hid=128, out=128):
        super().__init__()
        self.type_emb = nn.Embedding(type_vocab_size, hid)
        self.lin1 = nn.Linear(hid, hid)
        self.lin2 = nn.Linear(hid, out)
        self._node_type2id = {"<unk>":0}
    def _edges_and_types(self, code: str):
        try: tree = _pyast.parse(code)
        except Exception: return [], [0]
        nodes, edges = [], []
        q=[(tree,-1)]
        while q:
            node, parent = q.pop(0)
            nid=len(nodes); ntype = type(node).__name__
            tid = self._node_type2id.setdefault(ntype, len(self._node_type2id))
            nodes.append(tid)
            if parent>=0: edges.append((parent,nid)); edges.append((nid,parent))
            for ch in _pyast.iter_child_nodes(node): q.append((ch,nid))
        if not nodes: nodes=[0]
        return edges, nodes
    def forward_one(self, code: str, device):
        edges, type_ids = self._edges_and_types(code)
        N = len(type_ids)
        X = self.type_emb(torch.tensor(type_ids, device=device))
        if N == 1:
            return self.lin2(F.relu(self.lin1(X))).mean(dim=0)
        adj = torch.zeros((N,N), device=device)
        for i,j in edges:
            if 0<=i<N and 0<=j<N: adj[i,j]=1.0
        adj = adj + torch.eye(N, device=device)
        deg = adj.sum(dim=1); deg_inv_sqrt = torch.pow(deg.clamp(min=1.0), -0.5)
        D = torch.diag(deg_inv_sqrt); A = D @ adj @ D
        h = A @ X; h = F.relu(self.lin1(h)); z = A @ h; z = self.lin2(z)
        return z.mean(dim=0)
    def encode_batch(self, codes, device):
        zs = [self.forward_one(c, device) for c in codes]
        return F.normalize(torch.stack(zs, dim=0), dim=-1)
    @property
    def out_dim(self): return self.lin2.out_features

class FusionSiamese(nn.Module):
    def __init__(self, vocab_size, ast_out=128):
        super().__init__()
        self.enc = CodeEncoder(vocab_size=vocab_size)
        self.ast = ASTMiniGNN(out=ast_out)
        self.proj = nn.Linear(D_MODEL + ast_out, D_MODEL)
        self.tau  = nn.Parameter(torch.tensor(TAU_INIT, dtype=torch.float32), requires_grad=TAU_LEARNABLE)
    def fuse(self, ids, mask, raw_codes, device):
        z_txt = self.enc(ids, mask)
        z_ast = self.ast.encode_batch(raw_codes, device)
        z = torch.cat([z_txt, z_ast], dim=1)
        return F.normalize(self.proj(z), dim=-1)
    def forward(self, ids_a, mask_a, ids_b, mask_b, raw_a, raw_b):
        device = ids_a.device
        za = self.fuse(ids_a, mask_a, raw_a, device)
        zb = self.fuse(ids_b, mask_b, raw_b, device)
        return za, zb

# ===== 5) 스마트 로더 (프리픽스/네임스페이스/임베딩 shape 불일치 자동 처리) =====
def smart_load_fusion(model, ckpt_path, vocab_param_suffix="tok.weight"):
    sd = torch.load(ckpt_path, map_location="cpu")
    if isinstance(sd, dict) and "state_dict" in sd:
        sd = sd["state_dict"]
    def strip_prefix(k):
        for p in ["module.", "model."]:
            if k.startswith(p): return k[len(p):]
        return k
    new_sd = {}
    for k, v in sd.items():
        k0 = strip_prefix(k)
        for old_enc in ["siamese.enc.", "base.enc.", "encoder.", "model.enc."]:
            if k0.startswith(old_enc):
                k0 = "enc." + k0[len(old_enc):]
                break
        new_sd[k0] = v
    sd = new_sd

    # 현재 임베딩 파라미터 찾기
    embed_name, embed_param = None, None
    for name, p in model.named_parameters():
        if name.endswith("tok.weight"):
            embed_name, embed_param = name, p
            break
    # ckpt 임베딩 키 찾기
    ckpt_embed_key = None
    for k in sd.keys():
        if k.endswith(vocab_param_suffix) or k.endswith("tok.weight"):
            ckpt_embed_key = k; break

    # 임베딩 크기 다르면 부분 복사
    if embed_param is not None and ckpt_embed_key is not None:
        ckpt_W = sd[ckpt_embed_key]
        if ckpt_W.shape != embed_param.shape:
            print(f"[smart_load] embedding mismatch: ckpt {tuple(ckpt_W.shape)} vs model {tuple(embed_param.shape)}")
            with torch.no_grad():
                embed_param.data.zero_()
                n = min(ckpt_W.shape[0], embed_param.shape[0])
                d = min(ckpt_W.shape[1], embed_param.shape[1])
                embed_param.data[:n, :d].copy_(ckpt_W[:n, :d])
            del sd[ckpt_embed_key]  # shape 오류 방지
            print(f"[smart_load] partial copy done: {n}x{d}. new tokens random-init.")

    missing, unexpected = model.load_state_dict(sd, strict=False)
    print(f"[smart_load] strict=False → missing={len(missing)} unexpected={len(unexpected)}")
    if missing:    print("  missing (head/AST 등은 정상일 수 있음):", missing[:10], "...")
    if unexpected: print("  unexpected:", unexpected[:10], "...")
    return missing, unexpected

# ===== 6) Test Dataset =====
class TestPairs(Dataset):
    def __init__(self, frame, tok, max_len=512):
        self.df  = frame.reset_index(drop=True)
        self.tok = tok
        self.max_len = max_len
    def __len__(self): return len(self.df)
    def __getitem__(self, i):
        r = self.df.iloc[i]
        pid = r["pair_id"]
        code1 = "" if pd.isna(r["code1"]) else str(r["code1"])
        code2 = "" if pd.isna(r["code2"]) else str(r["code2"])
        ta = code_to_marked_text(code1)
        tb = code_to_marked_text(code2)
        ea = self.tok(ta, truncation=True, max_length=self.max_len,
                      padding="max_length", return_tensors="pt")
        eb = self.tok(tb, truncation=True, max_length=self.max_len,
                      padding="max_length", return_tensors="pt")
        return {
            "pair_id": pid,
            "ids_a":  ea["input_ids"].squeeze(0),
            "mask_a": ea["attention_mask"].squeeze(0),
            "ids_b":  eb["input_ids"].squeeze(0),
            "mask_b": eb["attention_mask"].squeeze(0),
            "raw_a": code1,  # AST-GNN
            "raw_b": code2,
        }

# ===== 7) 데이터 로드/검증 =====
assert os.path.exists(TEST_CSV), f"TEST_CSV not found: {TEST_CSV}"
test_df = pd.read_csv(TEST_CSV)

# 컬럼 이름 정규화: code1_norm/code2_norm -> code1/code2 로 맞춤
if {"code1_norm", "code2_norm"}.issubset(test_df.columns):
    test_df = test_df.rename(columns={"code1_norm": "code1", "code2_norm": "code2"})

need_cols = {"pair_id","code1","code2"}
assert need_cols.issubset(test_df.columns), f"Columns must contain {need_cols}, got {test_df.columns.tolist()}"
print("test shape:", test_df.shape)

test_dl = DataLoader(TestPairs(test_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE,
                     shuffle=False, num_workers=2, pin_memory=True)

# ===== 8) 모델 생성 & 가중치 로드 (자동 감지 버전; proj in/out 자동 일치) =====
assert os.path.exists(FINETUNE_CKPT), f"FINETUNE_CKPT not found: {FINETUNE_CKPT}"
model = build_and_load_fusion_auto(FINETUNE_CKPT, VOCAB_SIZE, pad_id=PAD_ID, ast_out=128)  # ast_out은 ASTMiniGNN(out=128)과 같아야 함
model.eval()

# ===== 9) 추론 & 저장 =====
USE_AMP = torch.cuda.is_available()
from torch.cuda.amp import autocast

pair_ids, probs = [], []
with torch.no_grad():
    for batch in tqdm(test_dl):
        ids_a  = batch["ids_a"].to(DEVICE, non_blocking=True)
        mask_a = batch["mask_a"].to(DEVICE, non_blocking=True)
        ids_b  = batch["ids_b"].to(DEVICE, non_blocking=True)
        mask_b = batch["mask_b"].to(DEVICE, non_blocking=True)
        raw_a  = batch["raw_a"]
        raw_b  = batch["raw_b"]
        with autocast(enabled=USE_AMP):
            za, zb = model(ids_a, mask_a, ids_b, mask_b, raw_a, raw_b)
            tau = model.tau if hasattr(model, "tau") else torch.tensor(0.1, device=za.device)
            logits = (za * zb).sum(dim=1) / torch.clamp(tau, min=1e-3)
            prob = torch.sigmoid(logits).detach().float().cpu()
        pair_ids.extend(batch["pair_id"])
        probs.extend(prob.tolist())

sub = pd.DataFrame({"pair_id": pair_ids, "similar": probs})
os.makedirs(os.path.dirname(SUBMIT_PATH), exist_ok=True)
sub.to_csv(SUBMIT_PATH, index=False)
print("Saved:", SUBMIT_PATH)
display(sub.head())